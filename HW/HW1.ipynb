{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7f948fc-b17b-4974-914a-81b5efa9f95c",
   "metadata": {},
   "source": [
    "# Homework 1: Machine Learning Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b47da4-4efa-4c3b-9ca4-e6be708fff36",
   "metadata": {},
   "source": [
    "### 1. Conditional Probability and the Chain Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbac57a5-383c-4a18-bc81-9f1eaf539e9f",
   "metadata": {},
   "source": [
    "Recall the definition of conditional probability: \n",
    " $$ P(A \\mid B) \\;=\\; \\frac{P(A \\cap B)}{P(B)} $$\n",
    "where $\\cap$ means \"intersection.\"\n",
    "##### a. Prove that $ P(A \\cap B \\cap C) \\;=\\; P(A \\mid B, C)\\,P(B \\mid C)\\,P(C) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92de3fbf-960d-489d-94f2-7f74b71d0c8d",
   "metadata": {},
   "source": [
    "$$ P(A \\cap (B \\cap C)) $$\n",
    "$$ = P(A \\mid B, C)P(B \\cap C) $$ \n",
    "$$ = P(A \\mid B, C)\\,P(B \\mid C)\\,P(C) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d14434-6664-4fb8-86a5-456a302401f5",
   "metadata": {},
   "source": [
    "##### b. Derive Bayes’ Theorem from the law of conditional probability, and define each term in the equation with a 1-sentence description."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b1d0f8-d5c4-413f-9896-7b105e53312d",
   "metadata": {},
   "source": [
    "The definition of conditional probability: \n",
    "$$ P(A \\mid B) \\;=\\; \\frac{P(A, B)}{P(B)} $$\n",
    "It is equally valid to say:\n",
    "$$ P(B \\mid A) \\;=\\; \\frac{P(A, B)}{P(A)} $$\n",
    "Solving for $ P(A, B) $ in the two equations allows $ P(A, B) $ to be expressed in two equivalent ways:\n",
    "$$ P(A \\mid B)P(B) = P(B \\mid A)P(A) $$\n",
    "Rearranging the above equation yields Bayes’ Theorem:\n",
    "$$ P(A \\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)} $$\n",
    "\n",
    "- $ P(A) $ and $ P(B) $ are the unconditional probabilities of those two events.\n",
    "- $P(B \\mid A)$ is the probability of $B$ given that we know $A$.\n",
    "- $P(A \\mid B)$ is the probability of $A$ given that we know $B$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f92dc7-4e6d-4202-b4c4-133208ca511a",
   "metadata": {},
   "source": [
    "### 2. Total Probability\n",
    "Let’s say I have two six-sided dice: one is fair, one is loaded. The loaded die has:\n",
    "$$\n",
    "P(x) =\n",
    "\\begin{cases}\n",
    "\\frac{1}{2}, & x = 6,\\\\\n",
    "\\frac{1}{10}, & x \\neq 6.\n",
    "\\end{cases}\n",
    "$$\n",
    "In addition to the two dice, I have a coin which I flip to determine which dice to roll. If\n",
    "the coin flip ends up heads I will roll the fair die, otherwise I’ll roll the loaded one. The\n",
    "probability that the coin flip is heads is $ p \\in [0, 1] $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465bc801-982e-499e-9ff0-bce55fbc7a0a",
   "metadata": {},
   "source": [
    "##### a. What is the expectation of the die roll, in terms of p?\n",
    "*Hint*: Recall that the expected value $ E[X] $ of a discrete random variable $ X $ (e.g., a coin flip) can be computed as\n",
    "$$ E[X] = \\sum_{i} x_i \\, P(X = x_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baed88f3-fe81-4cc1-a133-905067144dd0",
   "metadata": {},
   "source": [
    "The probability that the coin flips head is $p$, that it flips tails is $(1-p)$.\n",
    "Let $X$ be the die-roll outcome, and And $ E[X] $ be the expectation of the die roll.\n",
    "$$ E[X] = E[X \\mid Heads]P(Heads) + E[X \\mid Tails]P(Tails) $$\n",
    "$$ = (1 + 2 + 3 + 4 + 5 + 6)(\\frac{1}{6})p $$\n",
    "$$ + ((6)(\\frac{1}{2}) + (1 + 2 + 3 + 4 + 5)(\\frac{1}{10}))(1-p) $$\n",
    "$$ = \\frac{7}{2}p + \\frac{9}{2}(1-p)$$\n",
    "$$ = \\frac{9}{2} - p$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7d39f1-b238-4066-9584-9f64f3cc841c",
   "metadata": {},
   "source": [
    "##### b. What is the variance of the die roll, in terms of p?\n",
    "*Hint*: Recall that the variance $ Var(X) $ of a random variable X can be computed as $$ Var(X) = E[X^2] - (E[X])^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85e3c51-3446-42a9-86ec-b62d427fa3da",
   "metadata": {},
   "source": [
    "$$ E[X^2] = (1^2 + 2^2 + 3^2 + 4^2 + 5^2 + 6^2)(\\frac{1}{6})p + ((6^2)(\\frac{1}{2}) + (1^2 + 2^2 + 3^2 + 4^2 + 5^2)(\\frac{1}{10}))(1-p) $$\n",
    "$$ = \\frac{91}{6}p + \\frac{47}{2}(1-p) = \\frac{47}{2} - \\frac{25}{3}p$$\n",
    "$$ (E[X])^2 = (\\frac{9}{2} - p)^2 = \\frac{81}{4} - 9p + p^2 $$\n",
    "$$ Var(X) = \\frac{47}{2} - \\frac{25}{3}p - \\frac{81}{4} + 9p - p^2 $$\n",
    "$$ = \\frac{13}{4} + \\frac{2}{3}p - p^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edf000b-ad47-42c8-aada-762402c96620",
   "metadata": {},
   "source": [
    "### 3. Naive Bayes\n",
    "Consider the learning function $ f(X) \\to Y $, where class label $ Y \\in \\{T, F\\} $ and $ X = \\{x_1, x_2, \\ldots, x_n \\}$ , where $x_1$ is a boolean attribute and $ x_2, \\ldots, x_n $ are continuous attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aee60ea-533f-436c-b57f-08c51d02d62d",
   "metadata": {},
   "source": [
    "##### a. Assuming the continuous attributes are modeled as Gaussians, give and briefly explain the total number of parameters that you would need to estimate in order to classify a future observation using a Naive Bayes (NB) classifier.\n",
    "*Hint*: recall that a Naive Bayes classifier requires both the conditional probabilities $ P(X = x_i \\mid Y) $ and the class prior probability $ P(Y) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16defe12-fd67-446e-83c1-cb27cf1fcfab",
   "metadata": {},
   "source": [
    "1. One parameter is needed for the class prior, $P(Y)$.\n",
    "2. Two parameters are needed for boolean attribute $x_1$ (1 parameter per class)\n",
    "3. For the continuous attributes:\n",
    "   - There are $n - 1$ attributes\n",
    "   - Each univariate Gaussian has two parameters (mean and variance)\n",
    "   - Two parameters are needed for the two classes\n",
    "   - Thus, $ (2)(n-1)(2) = 4n - 4 $ parameters are needed.\n",
    "\n",
    "Summing each of these yields $ 4n - 1 $ total parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51fe8f2-7b48-414a-a852-f8470daa9080",
   "metadata": {},
   "source": [
    "##### b. How many more parameters would be required without the conditional independence assumption? No need for an exact number; an order of magnitude estimate will suffice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5047ffbc-442a-4b7e-8a90-f2481b36c95b",
   "metadata": {},
   "source": [
    "Without conditional independence, you would need\n",
    "- A mean vector of length $n-1$\n",
    "- A full covariance matrix with $ \\frac{(n-1)(n-1+1)}{2} $ entries\n",
    "\n",
    "Yielding a bound of $O(n^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e0c6b7-cca8-4408-80d6-c0281f1bf05a",
   "metadata": {},
   "source": [
    "### 4. Logistic Regression\n",
    "In Logistic Regression (LR), we assume the observations are independent of each other (not *conditionally* independent, just independent)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500d557d-bbf2-481b-b929-dce5cfe5fd50",
   "metadata": {},
   "source": [
    "##### a. Prove the decision boundary for Logistic Regression is linear. i.e., show that $ P(Y \\mid X) $ has the form:\n",
    "$$ w_0 + \\sum_{i} w_i X_i $$\n",
    "##### where $ Y \\in \\{0, 1 \\} $, and the quantity of the sum in the above equation will determine whether LR predicts 1 or 0.\n",
    "*Hint*: Recall that $$ P(Y = 0 \\mid X) \\;=\\; \\frac{1}{1 + \\exp\\!\\bigl(w_0 + \\sum_{i} w_i X_i\\bigr)} $$ and that $ P(Y=0 \\mid X) + P(Y = 1 \\min X) = 1 $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98446433-7c06-4308-8847-57ceb0433135",
   "metadata": {},
   "source": [
    "Let $p = P(Y | X) = P(Y=1 | X)$, the probability that $Y=1$ given the parameters $X$.<br>\n",
    "Let $l$ denote the logit function. $l$ is defined as $l = ln($odds$)$.<br>\n",
    "**odds** is the ratio of the probability $p$ to the probability $\\neg p$:\n",
    "$$ odds = \\frac{p}{1-p} = \\frac{P(Y=1 | X)}{P(Y=0 | X)}$$\n",
    "Given the definition of $P(Y = 0 \\mid X)$, the above can be simplified to:\n",
    "$$ odds = \\textrm{exp}(w_0 + \\sum_i w_i X_i)$$\n",
    "Plugging that in to the logit function yields the linear bound:\n",
    "$$ l = ln(\\textrm{exp}(w_0 + \\sum_i w_i X_i)) = w_0 + \\sum_i w_i X_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0f186e-116e-47f9-b6de-7802002332e7",
   "metadata": {},
   "source": [
    "##### b. *Briefly* describe one advantage and one disadvantage of LR compared to NB (two sentences total is plenty)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e810578-cfe9-4ecd-9cb7-badd878f7863",
   "metadata": {},
   "source": [
    "- Logistic regression learns the chance of each label directly from the data without treating features as independent, so it often gives more accurate predictions when features interact or don’t follow simple distributions\n",
    "- However, it must run an iterative optimization (e.g. gradient descent) to find its weights, which is slower and can overfit on small datasets, whereas Naive Bayes just counts occurrences and has a closed-form solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa0644d-1f40-4feb-872a-c26f4fa46bd9",
   "metadata": {},
   "source": [
    "### 5. Coding\n",
    "In this problem you will implement Logistic Regression (LR) for a document classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af79b67-0e30-4afc-98f6-d235e41c17d6",
   "metadata": {},
   "source": [
    "##### a. Imagine a certain word is never observed during training, but appears in a testing set. What will happen when the NB classifier predicts the probability of the word? Explain. Will LR have the same problem? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94c5ea7-e09d-4da3-a4be-8db9d032faff",
   "metadata": {},
   "source": [
    "- With a NB classifier, a new word would collapse the product of probabilities of all words to 0, meaning the model would not be able to distinguish between classes.\n",
    "- LR uses weights to classify words into one of two classes. A new word would simply be unweighted, and does not zero out the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98e8a7d-d7ec-4c7c-9c42-b7eec75599e3",
   "metadata": {},
   "source": [
    "##### b. Implement LR.\n",
    "This script should accept three arguments, in the following order:\n",
    "1. a file containing training data\n",
    "2. a file containing training labels\n",
    "3. a file containing testing data\n",
    "\n",
    "For training LR, we found a step size $\\eta$ around 0.0001 worked well.<br><br>\n",
    "The data files (train.data and test.data) contains three numbers on each line: **\\<document_id\\> \\<word_id\\> \\<count\\>**<br><br>\n",
    "Each row of the data files contains the count of how often a word (identified by ID)\n",
    "appears in a certain document. The corresponding label file for the training data has\n",
    "only one number per row of the file: the label, 1 or 0, of the document in the same row of the data file. <br> <br>\n",
    "For each line in the testing file, your code should print a predicted label (0 or 1) by itself on a single line.\n",
    "For example, if the following test.data file has four lines (words) in it, your\n",
    "program should print out four lines, each with either a 0 or a 1, e.g.\n",
    "```\n",
    "> python homework1.py train.data train.labels test.data\n",
    "0\n",
    "1\n",
    "1\n",
    "0\n",
    "```\n",
    "Don’t be alarmed if the training process of LR takes a few minutes; a good sanity checkis to make sure your weights are changing on each iteration (this can be a simple **print** statement). \n",
    "It is **highly recommended** that you use NumPy vectorized programming to train the weights efficiently.<br><br>\n",
    "Once you’ve tuned your script so it trains correctly and spits out a reasonable testing\n",
    "accuracy (should be substantially above random chance), give it a try on AutoLab! Just\n",
    "follow the submission instructions, and check your score on the scoreboard. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42f57ba6-6ab1-4426-bd9e-ab473d6b3b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ./data/train.label ---\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "\n",
      "--- ./data/test_partial.label ---\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "\n",
      "--- ./data/test_partial.data ---\n",
      "69 1 1\n",
      "68 2 1\n",
      "100 2 1\n",
      "122 2 1\n",
      "140 2 1\n",
      "\n",
      "--- ./data/train.data ---\n",
      "21 1 1\n",
      "41 2 1\n",
      "72 2 1\n",
      "100 2 1\n",
      "138 2 1\n"
     ]
    }
   ],
   "source": [
    "# Data exploration\n",
    "files = [\n",
    "    \"./data/train.label\",\n",
    "    \"./data/test_partial.label\",\n",
    "    \"./data/test_partial.data\",\n",
    "    \"./data/train.data\",\n",
    "]\n",
    "\n",
    "for path in files:\n",
    "    print(f\"\\n--- {path} ---\")\n",
    "    with open(path, \"r\") as f:\n",
    "        for _ in range(5):\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            print(line.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7b7b4de7-ce98-45af-9a55-8e49d30a6ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The data for this problem is drawn from the 20 Newsgroups data\n",
    "set. The training and test sets each contain 200 documents, 100 from\n",
    "comp.sys.ibm.pc.hardware (label 0) and 100 from comp.sys.mac.hardware\n",
    "(label 1). Each document is represented as a vector of word\n",
    "counts.\n",
    "\n",
    "The data consists of four files: train.data, train.label, test.data\n",
    "and test.label. The .data files contain word count matrices whose rows\n",
    "correspond to document_ids and whose columns correspond to\n",
    "word_ids. Each row of the .data files represents the number of times a\n",
    "certain word appeared in a certain document, in the following three\n",
    "column format:\n",
    "\n",
    "<document_id> <word_id> <count>\n",
    "\n",
    "The .label files simply list the class label for each document in\n",
    "order. i.e., the first entry of train.label is the label for the first\n",
    "document in train.data.\n",
    "\n",
    "You are also given PARTIAL testing sets. The testing sets provided are to give you an idea of how your classifier will perform on the full dataset.\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "STEP_SIZE = 0.0001\n",
    "\n",
    "train_data_file  = \"./data/train.data\"\n",
    "train_label_file = \"./data/train.label\"\n",
    "test_data_file   = \"./data/test_partial.data\"\n",
    "test_label_file = \"./data/test_partial.label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "452b3265-a7a6-4e30-9866-154351f2e77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in training data\n",
    "# labels\n",
    "y_train = np.loadtxt(train_label_file, dtype=int)\n",
    "\n",
    "# data\n",
    "train_triples  = np.loadtxt(train_data_file, dtype=int)\n",
    "doc_ids_train  = train_triples[:, 0] - 1   # zero-based\n",
    "word_ids_train = train_triples[:, 1] - 1\n",
    "counts_train   = train_triples[:, 2]\n",
    "\n",
    "n_train_docs = doc_ids_train.max() + 1\n",
    "vocab_size   = word_ids_train.max() + 1    # derive V from train set\n",
    "\n",
    "X_train = coo_matrix(\n",
    "    (counts_train, (doc_ids_train, word_ids_train)),\n",
    "    shape=(n_train_docs, vocab_size)\n",
    ").tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0767edde-b02d-4abb-931b-5393a90b1ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in testing data\n",
    "# 1) Load labels\n",
    "y_test = np.loadtxt(test_label_file, dtype=int)\n",
    "\n",
    "# 2) Load raw triples\n",
    "test_triples  = np.loadtxt(test_data_file, dtype=int)\n",
    "doc_ids_test  = test_triples[:, 0] - 1   # still 0-based, but possibly sparse\n",
    "word_ids_test = test_triples[:, 1] - 1\n",
    "counts_test   = test_triples[:, 2]\n",
    "\n",
    "# 3) Build a compact mapping of document IDs → consecutive rows\n",
    "unique_ids = np.unique(doc_ids_test)\n",
    "id2row     = {orig: new for new, orig in enumerate(unique_ids)}\n",
    "new_rows   = np.array([id2row[d] for d in doc_ids_test])\n",
    "\n",
    "# 4) Build the sparse test‐matrix with exactly len(unique_ids) rows\n",
    "X_test = coo_matrix(\n",
    "    (counts_test, (new_rows, word_ids_test)),\n",
    "    shape=(len(unique_ids), vocab_size)\n",
    ").tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "de5d0197-258c-4cfd-bdbb-9f0e2d58bbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "w = np.zeros(vocab_size)\n",
    "b = 0.0\n",
    "\n",
    "# define the sigma function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6d18a2a4-4579-4062-8705-46071e023bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0 — loss 0.693147 — update_norm 3.23e-02\n",
      "Epoch 1000 — loss 0.101985 — update_norm 1.21e-03\n",
      "Epoch 2000 — loss 0.059219 — update_norm 6.99e-04\n",
      "Epoch 3000 — loss 0.041680 — update_norm 4.95e-04\n",
      "Epoch 4000 — loss 0.032069 — update_norm 3.83e-04\n",
      "Epoch 5000 — loss 0.026007 — update_norm 3.12e-04\n",
      "Epoch 6000 — loss 0.021841 — update_norm 2.63e-04\n",
      "Epoch 7000 — loss 0.018807 — update_norm 2.27e-04\n",
      "Epoch 8000 — loss 0.016502 — update_norm 2.00e-04\n",
      "Epoch 9000 — loss 0.014692 — update_norm 1.78e-04\n",
      "Epoch 10000 — loss 0.013236 — update_norm 1.61e-04\n",
      "Epoch 11000 — loss 0.012038 — update_norm 1.47e-04\n",
      "Epoch 12000 — loss 0.011037 — update_norm 1.35e-04\n",
      "Epoch 13000 — loss 0.010188 — update_norm 1.24e-04\n",
      "Epoch 14000 — loss 0.009459 — update_norm 1.16e-04\n",
      "Epoch 15000 — loss 0.008826 — update_norm 1.08e-04\n",
      "Epoch 16000 — loss 0.008272 — update_norm 1.01e-04\n",
      "Epoch 17000 — loss 0.007783 — update_norm 9.53e-05\n",
      "Epoch 18000 — loss 0.007348 — update_norm 9.00e-05\n",
      "Epoch 19000 — loss 0.006958 — update_norm 8.53e-05\n",
      "Epoch 20000 — loss 0.006608 — update_norm 8.10e-05\n",
      "Epoch 21000 — loss 0.006290 — update_norm 7.72e-05\n",
      "Epoch 22000 — loss 0.006002 — update_norm 7.37e-05\n",
      "Epoch 23000 — loss 0.005739 — update_norm 7.04e-05\n",
      "Epoch 24000 — loss 0.005498 — update_norm 6.75e-05\n",
      "Epoch 25000 — loss 0.005276 — update_norm 6.48e-05\n",
      "Epoch 26000 — loss 0.005071 — update_norm 6.23e-05\n",
      "Epoch 27000 — loss 0.004881 — update_norm 6.00e-05\n",
      "Epoch 28000 — loss 0.004705 — update_norm 5.78e-05\n",
      "Epoch 29000 — loss 0.004542 — update_norm 5.58e-05\n",
      "Epoch 30000 — loss 0.004389 — update_norm 5.40e-05\n",
      "Epoch 31000 — loss 0.004246 — update_norm 5.22e-05\n",
      "Epoch 32000 — loss 0.004112 — update_norm 5.06e-05\n",
      "Epoch 33000 — loss 0.003986 — update_norm 4.90e-05\n",
      "Epoch 34000 — loss 0.003868 — update_norm 4.76e-05\n",
      "Epoch 35000 — loss 0.003756 — update_norm 4.62e-05\n",
      "Epoch 36000 — loss 0.003651 — update_norm 4.49e-05\n",
      "Epoch 37000 — loss 0.003551 — update_norm 4.37e-05\n",
      "Epoch 38000 — loss 0.003457 — update_norm 4.26e-05\n",
      "Epoch 39000 — loss 0.003367 — update_norm 4.15e-05\n",
      "Epoch 40000 — loss 0.003282 — update_norm 4.04e-05\n",
      "Epoch 41000 — loss 0.003201 — update_norm 3.94e-05\n",
      "Epoch 42000 — loss 0.003124 — update_norm 3.85e-05\n",
      "Epoch 43000 — loss 0.003051 — update_norm 3.76e-05\n",
      "Epoch 44000 — loss 0.002981 — update_norm 3.67e-05\n",
      "Epoch 45000 — loss 0.002914 — update_norm 3.59e-05\n",
      "Epoch 46000 — loss 0.002850 — update_norm 3.51e-05\n",
      "Epoch 47000 — loss 0.002789 — update_norm 3.44e-05\n",
      "Epoch 48000 — loss 0.002730 — update_norm 3.36e-05\n",
      "Epoch 49000 — loss 0.002674 — update_norm 3.30e-05\n",
      "Epoch 50000 — loss 0.002620 — update_norm 3.23e-05\n",
      "Epoch 51000 — loss 0.002568 — update_norm 3.17e-05\n",
      "Epoch 52000 — loss 0.002518 — update_norm 3.10e-05\n",
      "Epoch 53000 — loss 0.002471 — update_norm 3.05e-05\n",
      "Epoch 54000 — loss 0.002424 — update_norm 2.99e-05\n",
      "Epoch 55000 — loss 0.002380 — update_norm 2.93e-05\n",
      "Epoch 56000 — loss 0.002337 — update_norm 2.88e-05\n",
      "Epoch 57000 — loss 0.002296 — update_norm 2.83e-05\n",
      "Epoch 58000 — loss 0.002256 — update_norm 2.78e-05\n",
      "Epoch 59000 — loss 0.002217 — update_norm 2.73e-05\n",
      "Epoch 60000 — loss 0.002180 — update_norm 2.69e-05\n",
      "Epoch 61000 — loss 0.002144 — update_norm 2.64e-05\n",
      "Epoch 62000 — loss 0.002109 — update_norm 2.60e-05\n",
      "Epoch 63000 — loss 0.002075 — update_norm 2.56e-05\n",
      "Epoch 64000 — loss 0.002043 — update_norm 2.52e-05\n",
      "Epoch 65000 — loss 0.002011 — update_norm 2.48e-05\n",
      "Epoch 66000 — loss 0.001980 — update_norm 2.44e-05\n",
      "Epoch 67000 — loss 0.001950 — update_norm 2.41e-05\n",
      "Epoch 68000 — loss 0.001921 — update_norm 2.37e-05\n",
      "Epoch 69000 — loss 0.001893 — update_norm 2.34e-05\n",
      "Epoch 70000 — loss 0.001866 — update_norm 2.30e-05\n",
      "Epoch 71000 — loss 0.001840 — update_norm 2.27e-05\n",
      "Epoch 72000 — loss 0.001814 — update_norm 2.24e-05\n",
      "Epoch 73000 — loss 0.001789 — update_norm 2.21e-05\n",
      "Epoch 74000 — loss 0.001764 — update_norm 2.18e-05\n",
      "Epoch 75000 — loss 0.001741 — update_norm 2.15e-05\n",
      "Epoch 76000 — loss 0.001718 — update_norm 2.12e-05\n",
      "Epoch 77000 — loss 0.001695 — update_norm 2.09e-05\n",
      "Epoch 78000 — loss 0.001673 — update_norm 2.06e-05\n",
      "Epoch 79000 — loss 0.001652 — update_norm 2.04e-05\n",
      "Epoch 80000 — loss 0.001631 — update_norm 2.01e-05\n",
      "Epoch 81000 — loss 0.001611 — update_norm 1.99e-05\n",
      "Epoch 82000 — loss 0.001591 — update_norm 1.96e-05\n",
      "Epoch 83000 — loss 0.001572 — update_norm 1.94e-05\n",
      "Epoch 84000 — loss 0.001553 — update_norm 1.92e-05\n",
      "Epoch 85000 — loss 0.001535 — update_norm 1.89e-05\n",
      "Epoch 86000 — loss 0.001517 — update_norm 1.87e-05\n",
      "Epoch 87000 — loss 0.001499 — update_norm 1.85e-05\n",
      "Epoch 88000 — loss 0.001482 — update_norm 1.83e-05\n",
      "Epoch 89000 — loss 0.001465 — update_norm 1.81e-05\n",
      "Epoch 90000 — loss 0.001449 — update_norm 1.79e-05\n",
      "Epoch 91000 — loss 0.001433 — update_norm 1.77e-05\n",
      "Epoch 92000 — loss 0.001417 — update_norm 1.75e-05\n",
      "Epoch 93000 — loss 0.001402 — update_norm 1.73e-05\n",
      "Epoch 94000 — loss 0.001387 — update_norm 1.71e-05\n",
      "Epoch 95000 — loss 0.001372 — update_norm 1.69e-05\n",
      "Epoch 96000 — loss 0.001358 — update_norm 1.68e-05\n",
      "Epoch 97000 — loss 0.001343 — update_norm 1.66e-05\n",
      "Epoch 98000 — loss 0.001330 — update_norm 1.64e-05\n",
      "Epoch 99000 — loss 0.001316 — update_norm 1.62e-05\n",
      "Stopping at epoch 100000: update_norm=1.61e-05, rel_loss_change=1.01e-05\n"
     ]
    }
   ],
   "source": [
    "STEP_SIZE       = 0.0001\n",
    "TOL_UPDATE      = 1e-6     # ℓ₂‐norm threshold on the weight update\n",
    "TOL_LOSS_REL    = 1e-5     # relative change in loss threshold\n",
    "MAX_ITER        = 100000   # hard cap on iterations\n",
    "\n",
    "w = np.zeros(vocab_size)\n",
    "b = 0.0\n",
    "\n",
    "prev_loss = None\n",
    "epoch     = 0\n",
    "\n",
    "while True:\n",
    "    # 1) forward pass\n",
    "    z      = X_train.dot(w) + b\n",
    "    y_hat  = sigmoid(z)\n",
    "\n",
    "    # 2) compute gradients\n",
    "    delta  = y_train - y_hat\n",
    "    dw     = X_train.T.dot(delta)\n",
    "    db     = delta.sum()\n",
    "\n",
    "    # 2b) compute update norm\n",
    "    weight_update = STEP_SIZE * dw\n",
    "    update_norm   = np.linalg.norm(weight_update)\n",
    "\n",
    "    # 3) compute current loss\n",
    "    loss = -np.mean(y_train * np.log(y_hat) + (1-y_train) * np.log(1-y_hat))\n",
    "\n",
    "    # 4) check stopping criteria\n",
    "    cond_update = (update_norm < TOL_UPDATE)\n",
    "    cond_loss   = (prev_loss is not None and \n",
    "                   abs(loss - prev_loss) / prev_loss < TOL_LOSS_REL)\n",
    "    if cond_update or cond_loss or epoch >= MAX_ITER:\n",
    "        print(f\"Stopping at epoch {epoch}: \"\n",
    "              f\"update_norm={update_norm:.2e}, \"\n",
    "              f\"rel_loss_change={None if prev_loss is None else abs(loss-prev_loss)/prev_loss:.2e}\")\n",
    "        break\n",
    "\n",
    "    # 5) parameter update\n",
    "    w += weight_update\n",
    "    b += STEP_SIZE * db\n",
    "\n",
    "    # 6) logging\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch:4d} — loss {loss:.6f} — update_norm {update_norm:.2e}\")\n",
    "\n",
    "    prev_loss = loss\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "251b51e6-3dd2-4dc9-bdb1-3e04517364c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.5300\n",
      "Confusion matrix:\n",
      "[[ 0  0]\n",
      " [47 53]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000         0\n",
      "           1     1.0000    0.5300    0.6928       100\n",
      "\n",
      "    accuracy                         0.5300       100\n",
      "   macro avg     0.5000    0.2650    0.3464       100\n",
      "weighted avg     1.0000    0.5300    0.6928       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluation on test set ---\n",
    "\n",
    "# 1) Compute predicted probabilities\n",
    "z_test   = X_test.dot(w) + b\n",
    "y_prob   = sigmoid(z_test)\n",
    "\n",
    "# 2) Convert to binary predictions at threshold 0.5\n",
    "y_pred   = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "# 3) Compute accuracy\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Test accuracy: {accuracy:.4f}\")\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    log_loss\n",
    ")\n",
    "\n",
    "# 1) Confusion matrix and classification report (avoid warnings)\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(\n",
    "    y_test, \n",
    "    y_pred, \n",
    "    digits=4, \n",
    "    zero_division=0            # fill precision/recall with 0 instead of warning\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c33e641f-8ea0-46d7-8a6c-c3d1bde22e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5000\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 0  0]\n",
      " [50 50]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000         0\n",
      "           1     1.0000    0.5000    0.6667       100\n",
      "\n",
      "    accuracy                         0.5000       100\n",
      "   macro avg     0.5000    0.2500    0.3333       100\n",
      "weighted avg     1.0000    0.5000    0.6667       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare against scikit-learn’s LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    log_loss\n",
    ")\n",
    "\n",
    "# 1) Instantiate and train\n",
    "clf = LogisticRegression(\n",
    "    penalty='l2',       # ℓ2 regularization\n",
    "    C=1.0,               # inverse regularization strength\n",
    "    solver='lbfgs',      # good default for small-to-medium datasets\n",
    "    max_iter=100000,\n",
    "    random_state=0\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 2) Make predictions\n",
    "y_pred_prob = clf.predict_proba(X_test)[:, 1]   # probability for class “1”\n",
    "y_pred      = clf.predict(X_test)\n",
    "\n",
    "# 3) Evaluate\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "print(\"\\nConfusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    digits=4,\n",
    "    zero_division=0\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dc58f2-0cbd-452e-b689-d9824fbcdf58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
