{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a821286-4258-4eb1-91da-ead7c66091ad",
   "metadata": {},
   "source": [
    "# Homework 5: Everything Old Is New Again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205bc915-12ff-4b55-8f96-d51b747ff011",
   "metadata": {},
   "source": [
    "### 1. Neural Networks\n",
    "\n",
    "In this question we’ll look at some of the basic properties of feed-forward neural networks.\n",
    "\n",
    "First, consider the myriad activation functions available to neural networks. In this problem, we’ll only look at very simple ones, starting with a combination of linear activation functions and the hard threshold; specifically, where the output of a node y is either 1 or 0: $$ y = \\begin{cases} 1 & \\text{if} \\;\\; w_{0} + \\sum_{i}w_{i}x_{i} \\ge 0 \\\\ 0 & \\text{otherwise}. \\end{cases} $$\n",
    "\n",
    "Which of the following functions can be exactly represented by a neural network with one hidden layer, and which uses linear and/or hard threshold activation functions? For each case, briefly justify your answer (sketching an example is fine)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7e40fe-0be8-4724-b2b6-9abb7b3e5c33",
   "metadata": {},
   "source": [
    "##### a. Polynomials of degree one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56b0339-7119-436e-b479-911ab740e981",
   "metadata": {},
   "source": [
    "- Yes; the network’s forward pass is literally a linear polynomial\n",
    "- If there needs to be a hidden layer, use a linear unit and composition keeps the function linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b89ec1b-2dc8-4ee7-bc34-126f225b02bb",
   "metadata": {},
   "source": [
    "##### b. Hinge loss $h(x) = \\max(1-x, 0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1109cc9-2980-4ace-a9d2-c7202384931e",
   "metadata": {},
   "source": [
    "- One hidden layer of linear + hard-threshold units can give a flat line, a pure staircase, or a staircase sitting on a single slanted line, but it cannot make the shape of the hinge loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3953c5a0-f867-44f4-9659-4a79a9a0b278",
   "metadata": {},
   "source": [
    "##### c. Polynomials of degree two."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a586f6a-5d3a-4e5c-8f9e-0da0c9c34f21",
   "metadata": {},
   "source": [
    "- cannot model the continuos change of this polynomial given the hard desicion boundaries that this NN can provide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597ae8cd-a74c-4183-a591-12fb386ad989",
   "metadata": {},
   "source": [
    "##### d. Piecewise constant functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cf74a5-d0f3-4e9f-92d2-ea7d778385d0",
   "metadata": {},
   "source": [
    "- yes, linear unit + hard threshold = piecwise constant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08c17b1-1f83-42c9-8a6e-dca1e98c0f27",
   "metadata": {},
   "source": [
    "Consider the following XOR-like function in two-dimensional space: $$ f(x_{1}, x_{2}) = \\begin{cases} 1 & x_{1}, x_{2} \\ge 0 \\;\\; \\text{or} \\;\\; x_{1}, x_{2} \\lt 0 \\\\ -1 & \\text{otherwise} \\end{cases} $$\n",
    "\n",
    "We want to represent this function with a neural network. For some reason, we decide we only want to use the threshold activation function for the hidden units and output unit: $$ h_{\\theta}(v) = \\begin{cases} 1 & v \\ge \\theta \\\\ -1 & \\text{otherwise}  \\end{cases} $$\n",
    "\n",
    "##### e. Show that the smallest number of hidden layers needed to represent this XOR function is two. Give a neural network with two hidden layers of threshold functions that represent f, the XOR function. Again, you are welcome to provide a drawing, but that drawing must include values being propagated from each neuron. Alternatively, you could draw a table showing the values at each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6bb712-cc4e-49a8-96a0-cb83c2196760",
   "metadata": {},
   "source": [
    "| layer                   | neuron                   | $(w,b)$                                          | output when $(x_1,x_2)$ is … |\n",
    "| ----------------------- | ------------------------ | ------------------------------------------------ | ---------------------------- |\n",
    "| **Input → Hidden 1**    | $h_1$                    | $(\\,[1,0],\\;0)$                                  | 1  1  -1  -1                 |\n",
    "|                         | $h_2$                    | $(\\,[0,1],\\;0)$                                  | 1  -1  1  -1                 |\n",
    "| **Hidden 1 → Hidden 2** | $g_1$ detects **both +** | $(\\,[1,1],\\;\\!-1.5)$                             | 1  -1  -1  -1                |\n",
    "|                         | $g_2$ detects **both -** | $(\\,[\\!-1,-1],\\;\\!-1.5)$                         | -1  -1  -1  1                |\n",
    "| **Hidden 2 → Output**   | $y$                      | $(\\,[1,1],\\;0.5)$ (i.e. threshold $\\theta=-0.5$) | **1  -1  -1  1**             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3d9959-eee9-410d-89d8-a2707244919a",
   "metadata": {},
   "source": [
    "### 2. Hierarchical Clustering\n",
    "\n",
    "We spent a little bit of time in class discussing hierarchical clustering, of which–fun fact–graph-based segmentation algorithms (min-cut, norm-cut, and even spectral clustering variants) are often considered a part. We primarily discussed two variants: *top-down*, in which all data points start out in a single large cluster that is continually split until a stopping criterion is reached, and *bottom-up*, in which all data points start out in their own clusters that are continually merged until a stopping criterion is reached.\n",
    "\n",
    "Here, we’ll explore the latter, also known as agglomerative clustering. The basic algorithm is as follows:\n",
    "1. Start with each point in a cluster of its own\n",
    "2. Until there is only one cluster\n",
    "    1. Find closest pair of clusters\n",
    "    2. Merge them\n",
    "3. Return the tree of cluster-mergers\n",
    "\n",
    "To convert this procedure into an implementation, one only needs to be able to quantify how “close” two clusters are. While not mentioned in detail, we did discuss metrics that define distance between two clusters, such as single-link, complete-link, and average-link.\n",
    "\n",
    "In this problem, you’ll look at an alternative approach to quantifying the distance between two disjoint clusters, proposed by Joe H. Ward in 1963. We will call it **Ward’s metric**.\n",
    "\n",
    "Ward’s metric simply says that the distance between two disjoint clusters, $X$ and $Y$, is how much the sum of squares will increase when we merge them. More formally: $$ \\Delta(X, Y) = \\sum_{i \\in X \\cup Y}\\| \\vec{x}_{i} - \\vec{\\mu}_{X \\cup Y} \\|^{2} - \\sum_{i \\in X}\\| \\vec{x}_{i} - \\vec{\\mu}_{X} \\|^{2} - \\sum_{i \\in Y}\\| \\vec{x}_{i} - \\vec{\\mu}_{Y} \\|^{2} $$ where $\\vec{\\mu}_{Z}$ is the centroid of cluster $Z$, and $\\vec{x}_{i}$ is a data point in our corpus. Here, $\\Delta(X, Y)$ is considered the *merging cost* of combining clusters $X$ and $Y$ into one cluster, $X \\cup Y$. That is, on each iteration, the two clusters with the lowest *merging cost* is merged using Ward’s metric as a distance measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7e7431-25b9-45b4-88e4-fad4166afe45",
   "metadata": {},
   "source": [
    "##### a. Can you reduce the formula given for $\\Delta(X, Y)$ to a simpler form? Provide the simplified formula and the steps to get there. Your formula should be in terms of the cluster sizes (i.e., the number of points in a given cluster, denoted as $n_{X}$ and $n_{Y}$) and the distance $\\| \\vec{\\mu}_{X} - \\vec{\\mu}_{Y} \\|^{2}$ between cluster centroids $\\vec{\\mu}_{X}$ and $\\vec{\\mu}_{Y}$ (yes, **only** the $n_{X}$, $n_{Y}$, $\\vec{\\mu}_{X}$, and $\\vec{\\mu}_{Y}$ symbols should be in your final equation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f006cbc2-6a7f-489c-9dff-a802a542340a",
   "metadata": {},
   "source": [
    "$$ \\sum_{i \\in X \\cup Y}\\| \\vec{x}_{i} - \\vec{\\mu}_{X \\cup Y} \\|^{2} = \\sum_{i \\in X \\cup Y}\\| \\vec{x}_{i} \\|^{2} - n_{X \\cup Y}\\| \\vec{\\mu}_{X \\cup Y} \\|^{2} $$ \n",
    "$$ \\sum_{i \\in X}\\| \\vec{x}_{i} - \\vec{\\mu}_{X} \\|^{2} = \\sum_{i \\in X}\\| \\vec{x}_{i} \\|^{2} - n_{X}\\| \\vec{\\mu}_{X} \\|^{2} $$\n",
    "$$ \\sum_{i \\in Y}\\| \\vec{x}_{i} - \\vec{\\mu}_{Y} \\|^{2} = \\sum_{i \\in Y}\\| \\vec{x}_{i} \\|^{2} - n_{Y}\\| \\vec{\\mu}_{Y} \\|^{2} $$\n",
    "$$ \\Delta(X, Y) = \\sum_{i \\in X \\cup Y}\\| \\vec{x}_{i} \\|^{2} - n_{X \\cup Y}\\| \\vec{\\mu}_{X \\cup Y} \\|^{2} - \\sum_{i \\in X}\\| \\vec{x}_{i} \\|^{2} + n_{X}\\| \\vec{\\mu}_{X} \\|^{2} - \\sum_{i \\in Y}\\| \\vec{x}_{i} \\|^{2} + n_{Y}\\| \\vec{\\mu}_{Y} \\|^{2} $$\n",
    "$$ \\Delta(X, Y) = - n_{X \\cup Y}\\| \\vec{\\mu}_{X \\cup Y} \\|^{2} + n_{X}\\| \\vec{\\mu}_{X} \\|^{2} + n_{Y}\\| \\vec{\\mu}_{Y} \\|^{2} $$\n",
    "$$ \\mu_{X \\cup Y} = \\frac{n_{X}\\mu_{X}+n_{Y}\\mu_{Y}}{n_{X}+n_{Y}} $$\n",
    "$$ \\Delta(X, Y) = \\frac{n_{X}n_{Y}}{n_{X} + n_{Y}}\\| \\mu_{X} - \\mu_{Y} \\|^{2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2624e08-9c20-4889-bec8-54553c0f0725",
   "metadata": {},
   "source": [
    "##### b. Assume you are given two *pairs* of clusters $P_{1}$ and $P_{2}$. The centers of the two clusters in the $P_{1}$ pair are farther apart than the pair of centers in $P_{2}$. Using Ward’s metric, does agglomerative clustering *always* choose to merge the two clusters in $P_{2}$? Why or why not? Justify your answer with a simple example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b93a32-126f-4a7d-97ab-24d2cc5d7f52",
   "metadata": {},
   "source": [
    "- No, since Ward's metric also depends on cluster sizes, not just distance between centriods\n",
    "| pair   | cluster sizes | centroid gap         | Ward cost                               |\n",
    "| ------ | ------------- | -------------------- | --------------------------------------- |\n",
    "| **P₁** | $n_X=n_Y=1$   | $\\|\\mu_X-\\mu_Y\\|=10$ | $\\tfrac{1\\cdot1}{1+1}\\,10^{2}=50$       |\n",
    "| **P₂** | $n_X=n_Y=100$ | $\\|\\mu_X-\\mu_Y\\|=5$  | $\\tfrac{100\\cdot100}{200}\\,5^{2}=1 250$ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f96f52-6e6d-4237-af4a-ecccadc7de9f",
   "metadata": {},
   "source": [
    "### 3. Genetic Programming\n",
    "\n",
    "In this part, you’ll re-implement your logistic regression code from the Homework 1 (we’ve gone full circle!) to use a simple genetic algorithm to learn the weights, instead of gradient descent. Provided is a skeleton script that sets the following:\n",
    "1. a file containing training data\n",
    "2. a file containing training labels\n",
    "3. a file containing testing data\n",
    "\n",
    "And optionally sets:\n",
    "- `n`: a population size (default: 200)\n",
    "- `s`: a per-generation survival rate (default: 0.3)\n",
    "- `m`: a mutation rate (default: 0.05)\n",
    "- `g`: a maximum number of generations (default: 50)\n",
    "- `r`: a random seed (default: -1)\n",
    "\n",
    "Your evolutionary algorithm for learning the weights should have a few core components:\n",
    "- **Random population initialization.** You should initialize a full array of weights *randomly* (don’t use all 0s!); this counts as a single “person” in the full population. Consequently, initialize $n$ arrays of weights randomly for your full population. You’ll evaluate each of these weights arrays independently and pick the best-performing ones to carry on to the next generation.\n",
    "- **Fitness function.** This is a way of evaluating how “good” your current solution is. Fortunately, we have this already: the objective function! You can use the weights to predict the training labels (as you did during gradient descent); the fitness for a set of weights is then the *average classification accuracy*.\n",
    "- Reproduction. Once you’ve evaluated the fitness of your current population, you’ll use that information to evolve the “strongest.” You’ll first take the top $s$%–the $ns$ arrays of weights with the highest fitness scores–and set them aside as the “parents” of the next generation. Then, you’ll “breed” random pairs of these parents parents to produce “children” until you have $n$ arrays of weights again. The breeding is done by simply averaging the two sets of parent weights together.\n",
    "- **Mutation.** Each individual weight has a mutation rate of $m$. Once you’ve computed the “child” weight array from two parents, you need to determine where and how many of the elements in the child array will mutate. First, flip a coin that lands on heads (i.e., indicates mutation) with probability $m$ (the mutation rate) for each weight $w_{i}$. Then, for each mutation, you’ll generate the new $w_{i}$ by sampling from a Gaussian distribution with mean and variance set to be the empirical mean and variance of *all* the $w_{i}$ weights of the *previous* generation. So if $W_{p}$ is the $n \\times |\\beta|$ matrix of the previous population of weights, then we can define $\\mu_{i} = W_{p}[:, i]\\text{.mean()}$ and $\\sigma_{i}^{2}$ $=$ $W_{p}[:, i]\\text{.var()}$ Using these quantities, we can then draw our new weight $w_{i} \\sim \\mathcal{N}(\\mu_{i}, \\sigma_{i}^{2})$.\n",
    "- **Generations.** You’ll run the fitness evaluation, reproduction, and mutation repeatedly for $g$ generations, after which you’ll take the set of weights from the final population with the highest fitness and evaluate these weights against the testing dataset.\n",
    "- The parent and child populations should be kept *distinct* during reproduction, and only the children should undergo mutation!\n",
    "\n",
    "The data files (`train.data` and `test.data`) contains three numbers on each line: **\\<document_id\\> \\<word_id\\> \\<count\\>**\n",
    "\n",
    "Each row of the data files contains the count of how often a given word (identified by ID) appears in certain documents (also identified by ID). The corresponding labels for the data has only one number per row in the file: the label, 1 or 0, of the document with ID corresponding to the row of the label in the label file. For example, a 0 on the 27th line of the label file means the document with ID 27 has the label 0.\n",
    "\n",
    "After you’ve found your final weights and used them to make predictions on the test set, your code should out predicted labels (0 or 1) by itself on a single line, *one for each document*–this means a single line of output per unique document ID (or per line in one of the `.label` files) to an output file. For example, if the following `test.data` file has four unique document IDs in it, your program should print out four lines, each with a 1 or 0 on it, e.g.:\n",
    "```\n",
    "train_data_file  = \"./data/train.data\"\n",
    "train_label_file = \"./data/train.label\"\n",
    "test_data_file   = \"./data/test.data\"\n",
    "...\n",
    "\n",
    "--- ./data/testPredicted.label ---\n",
    "0\n",
    "0\n",
    "1\n",
    "1\n",
    "```\n",
    "Then, compare to the test labels and compute the accuracy and classification report.\n",
    "\n",
    "**NOTE 1**: Evolutionary programs **will take longer** than logistic regression’s gradient descent. I strongly recommend staying under a population size of 300, with no more than about 300 generations. **Make liberal use of NumPy vectorized programming** to ensure your program is running as efficiently as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc9cd766-9bac-4e62-a94f-169c2a06d7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ./data/train.data ---\n",
      "21 1 1\n",
      "41 2 1\n",
      "72 2 1\n",
      "100 2 1\n",
      "138 2 1\n",
      "\n",
      "--- ./data/train.label ---\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "\n",
      "--- ./data/test_partial.data ---\n",
      "1 10 3\n",
      "1 21 4\n",
      "1 32 1\n",
      "1 36 1\n",
      "1 39 1\n",
      "\n",
      "--- ./data/test_partial.label ---\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Data exploration\n",
    "files = [\n",
    "    \"./data/train.data\",\n",
    "    \"./data/train.label\",\n",
    "    \"./data/test_partial.data\",\n",
    "    \"./data/test_partial.label\"\n",
    "]\n",
    "\n",
    "for path in files:\n",
    "    print(f\"\\n--- {path} ---\")\n",
    "    with open(path, \"r\") as f:\n",
    "        for _ in range(5):\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            print(line.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7f43f6a-0f6d-4071-a720-db59a3fc32d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_data_file  = \"./data/train.data\"\n",
    "train_label_file = \"./data/train.label\"\n",
    "test_data_file   = \"./data/test_partial.data\"\n",
    "test_label_file = \"./data/test_partial.label\"\n",
    "output_file = \"./data/testPredicted.label\"\n",
    "\n",
    "n = 200 # population size\n",
    "s = 0.3 # per-generation survival rate\n",
    "m = 0.05 # mutation rate\n",
    "g = 50 # max number of generations\n",
    "r = -1 # random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a59e105-3d50-4711-a154-f53fad76aedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_X(path):\n",
    "    # Load the data.\n",
    "    mat = np.loadtxt(path, dtype = int)\n",
    "    max_doc_id = mat[:, 0].max()\n",
    "    max_word_id = mat[:, 1].max()\n",
    "    X = np.zeros(shape = (max_doc_id, max_word_id))\n",
    "    for (docid, wordid, count) in mat:\n",
    "        X[docid - 1, wordid - 1] = count\n",
    "    return X\n",
    "\n",
    "def _load_data(data, labels):\n",
    "    # Load the labels.\n",
    "    y = np.loadtxt(labels, dtype = int)\n",
    "    X = _load_X(data)\n",
    "\n",
    "    # Return.\n",
    "    return [X, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "922036ee-9b43-4dac-b563-55f89a53c327",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1321/3583838077.py:6: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1.0 + np.exp(-z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.8300\n",
      "Test  accuracy: 0.5500\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.556     0.500     0.526        50\n",
      "           1      0.545     0.600     0.571        50\n",
      "\n",
      "    accuracy                          0.550       100\n",
      "   macro avg      0.551     0.550     0.549       100\n",
      "weighted avg      0.551     0.550     0.549       100\n",
      "\n",
      "\n",
      "Predictions written to ./data/testPredicted.label\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# ------------------------------------------------------------------ utilities --\n",
    "def _sigmoid(z):\n",
    "    \"\"\"Vectorised logistic.\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def _predict_labels(X, w):\n",
    "    \"\"\"0/1 predictions from weight vector w.\"\"\"\n",
    "    return (_sigmoid(X @ w) >= 0.5).astype(int)\n",
    "\n",
    "def _fitness(pop, X, y):\n",
    "    \"\"\"Classification-accuracy fitness for every weight vector in pop (shape n×d).\"\"\"\n",
    "    preds = _sigmoid(X @ pop.T) >= 0.5          # (n_samples, n_individuals)\n",
    "    return (preds.T == y).mean(axis=1)          # (n_individuals,)\n",
    "\n",
    "def _pad_features(X, d_target):\n",
    "    \"\"\"Right-pad feature matrix with zeros to reach d_target columns.\"\"\"\n",
    "    if X.shape[1] < d_target:\n",
    "        pad = np.zeros((X.shape[0], d_target - X.shape[1]), dtype=X.dtype)\n",
    "        X   = np.hstack([X, pad])\n",
    "    return X\n",
    "\n",
    "# ---------------------------------------------------------- load & harmonise --\n",
    "X_train, y_train = _load_data(train_data_file,  train_label_file)\n",
    "X_test , y_test  = _load_data(test_data_file ,  test_label_file)\n",
    "\n",
    "d = max(X_train.shape[1], X_test.shape[1])      # global vocab / feature size\n",
    "X_train = _pad_features(X_train, d)\n",
    "X_test  = _pad_features(X_test , d)\n",
    "\n",
    "# ---------------------------------------- GA initialisation (population W^0) --\n",
    "rng = np.random.default_rng(None if r < 0 else r)\n",
    "pop = rng.standard_normal(size=(n, d))          # N(0,1)  initial weights\n",
    "\n",
    "# ----------------------------------------------------------- evolutionary loop --\n",
    "num_parents = max(1, int(round(s * n)))\n",
    "\n",
    "for gen in range(g):\n",
    "    # -------- fitness evaluation\n",
    "    fit = _fitness(pop, X_train, y_train)\n",
    "\n",
    "    # -------- selection\n",
    "    parent_idx = np.argsort(fit)[-num_parents:]     # top-s% survive\n",
    "    parents    = pop[parent_idx]\n",
    "\n",
    "    # -------- reproduction  (children = averaged random parent pairs)\n",
    "    kids_needed = n - num_parents\n",
    "    idx1 = rng.integers(0, num_parents, size=kids_needed)\n",
    "    idx2 = rng.integers(0, num_parents, size=kids_needed)\n",
    "    children = 0.5 * (parents[idx1] + parents[idx2])\n",
    "\n",
    "    # -------- mutation  (children only)\n",
    "    mu_prev    = pop.mean(axis=0)\n",
    "    sigma_prev = pop.var(axis=0, ddof=0)\n",
    "    mutate_mask = rng.random(size=children.shape) < m\n",
    "    if mutate_mask.any():\n",
    "        children[mutate_mask] = rng.normal(\n",
    "            loc  = mu_prev[np.newaxis, :],\n",
    "            scale= np.sqrt(sigma_prev + 1e-12)[np.newaxis, :],\n",
    "            size = children.shape\n",
    "        )[mutate_mask]\n",
    "\n",
    "    # -------- next generation = parents (unmutated) ∪ mutated children\n",
    "    pop = np.vstack([parents, children])\n",
    "\n",
    "# ---------------------------------------------------- best individual & eval --\n",
    "best_w   = pop[_fitness(pop, X_train, y_train).argmax()]\n",
    "\n",
    "y_hat_tr = _predict_labels(X_train, best_w)\n",
    "y_hat_ts = _predict_labels(X_test , best_w)\n",
    "\n",
    "print(f\"Train accuracy: {accuracy_score(y_train, y_hat_tr):.4f}\")\n",
    "print(f\"Test  accuracy: {accuracy_score(y_test , y_hat_ts):.4f}\\n\")\n",
    "print(classification_report(y_test, y_hat_ts, digits=3))\n",
    "\n",
    "# ------------------------------------------------------- write predictions ----\n",
    "with open(output_file, \"w\") as f_out:\n",
    "    for p in y_hat_ts:\n",
    "        f_out.write(f\"{int(p)}\\n\")\n",
    "\n",
    "print(f\"\\nPredictions written to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751ba1fd-b45a-4dac-9643-87a952fa52f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
