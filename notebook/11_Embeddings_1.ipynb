{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f7daa93-dcbd-4f4f-92bc-27dcfe753753",
   "metadata": {},
   "source": [
    "# Embeddings 1: Metric Learning\n",
    "\n",
    "## Previously...\n",
    "- Spectral Clustering\n",
    "1. Define graph (affinities $\\rightarrow$ Laplacian)\n",
    "2. Compute eigenvectors (*embedding*)\n",
    "3. Cluster embeddings trivially ($k$-means)\n",
    "\n",
    "## Embeddings\n",
    "- What is an *embedding*?\n",
    "$$ f : X \\to Y $$\n",
    "    - Mapping\n",
    "    - Transformation\n",
    "    - Reveals/preserves \"structure\"\n",
    "\n",
    "![visual showing raw  high-dimensional inputs --(embedding)--> the resulting 2D scatter-plot where points/clusters are colored](./pics/embedding_visual.png)\n",
    "\n",
    "- From Vicki Boykis’ “What Are Embeddings” (2023)\n",
    "    - ***Transforms*** multimodal input into representations that are easier to perform intensive computation on, in the form of **vectors**, tensors, or graphs. For the purpose of machine learning, we can think of vectors as a list (or array) of numbers.\n",
    "    - ***Compresses*** input information for use in a machine learning **task** - the type of methods available to us in machine learning to solve specific problems - such as summarizing a document or identifying tags or labels for social media posts or performing **semantic search** on a large text corpus. The process of compression changes variable feature dimensions into fixed inputs, allowing them to be passed efficiently into downstream components of machine learning systems.\n",
    "    - ***Creates (represents) an embedding space*** that is specific to the data the embeddings were trained on but that, in the case of deep learning representations, can also generalize to other tasks and domains through **transfer learning** - the ability to switch contexts - which is one of the reasons embeddings have exploded in popularity across machine learning applications.\n",
    "\n",
    "## Embedding Strategies (non-DL)\n",
    "- Principal Components Analysis (PCA) : Linear\n",
    "    - Sparse & Kernel PCA : Sparse & Nonlinear\n",
    "- Independent Components Analysis (ICA) : Non-Gaussian\n",
    "- Non-negative Matrix Factorization (NMF) : Non-negative\n",
    "- Locally-linear Embeddings (LLE) : Nonlinear\n",
    "- Dictionary Learning : Sparse & Nonlinear\n",
    "\n",
    "## Million dollar question: How to know the embedding is right?\n",
    "- If you’re performing **classification**, it’s pretty easy to know if your embedding is “right”\n",
    "    - Error decreases? $\\rightarrow$ right!\n",
    "    - Error increases? $\\rightarrow$ wrong!\n",
    "- what about for **unsupervised learning**?\n",
    "\n",
    "## Assumptions\n",
    "- Choice of embedding $\\rightarrow$ assumptions about the data\n",
    "- **What if we knew something about the data?**\n",
    "- **\"Side information\"**: we  don’t know what classes/clusters the data belong to, but we do have some notion of similarity\n",
    "\n",
    "## Side information\n",
    "- Define a set $S$\n",
    "    - for every pair $x_i$ and $x_j$ that are similar, we put this pair in $S$\n",
    "- \"Similar\" is user-defined; can mean anything\n",
    "- Likewise have a set $D$\n",
    "    - for every pair $x_i$ and $x_j$ that are dissimilar, we put this pair in $D$\n",
    "    - can consist of every pair not in $S$, or specific pairs if information is available\n",
    "- We have this similarity information; what can we do with it?\n",
    "\n",
    "## Distance metrics\n",
    "- Goal: use side-information to **learn a new distance metric**\n",
    "- Encode our side-information in a “metric” $A$\n",
    "- Generalization of Euclidean distance\n",
    "    - Note when $A = I$, this is regular Euclidean distance\n",
    "    - When $A$ is diagonal, this is a “weighted” Euclidean distance\n",
    "    - When data are put through nonlinear basis functions $\\phi$, nonlinear metrics can be learned\n",
    "$$ \n",
    "d(\\vec{x}, \\vec{y}) = d_{A}(\\vec{x}, \\vec{y}) \n",
    "= \\| \\vec{x} - \\vec{y} \\|_{A} \n",
    "= \\sqrt{(\\vec{x} - \\vec{y})^{T}A(\\vec{x} - \\vec{y})} \n",
    "= \\sqrt{(\\phi(\\vec{x}) - \\phi(\\vec{y}))^{T}A(\\phi(\\vec{x}) - \\phi(\\vec{y}))} \n",
    "$$\n",
    "\n",
    "- Quick review: **What constitutes a *valid distance* metric?**\n",
    "    1. Non negativity $$ d(\\vec{x}, \\vec{y}) \\ge 0 $$\n",
    "    2. Symmetry $$ d(\\vec{x}, \\vec{y}) = d(\\vec{y}, \\vec{x}) $$\n",
    "    3. Triangle Inequality $$ d(\\vec{x}, \\vec{x}) \\le d(\\vec{x}, \\vec{y}) + d(\\vec{y}, \\vec{z}) $$\n",
    "    4. (**Not valid, \"Pseudometric\"**) Identity of indiscernibles $$ d(\\vec{x}, \\vec{y}) = 0 \\iff x = y $$\n",
    "\n",
    "## Form of a metric\n",
    "- Learning metric $A$ ($G$ in figure) also equivalent to replacing each point $x$ with $A^{\\frac{1}{2}}x$ and using standard Euclidean distance\n",
    "- **It’s an embedding!**\n",
    "- Learning a **space** inhabited by your data\n",
    "    - Bonus: easy to incorporate new data! (unlike LLE or others)\n",
    "\n",
    "![“proof by picture” that learning a metric is equivalent to embedding your points via the square‐root of G and then using ordinary Euclidean distance. shows three cases: G=I, G diagonal, and G full](./pics/formOfMetric_visual.png)\n",
    "\n",
    "## Learning a metric (1)\n",
    "- Goal: Define a metric A that respects constraint sets $S$ and $D$\n",
    "- Simple enough: constrain all pairs in $S$ to have small distances\n",
    "$$ \\min_{A}\\sum_{\\vec{x},\\,\\vec{y}\\in S}\\bigl\\lVert \\vec{x} - \\vec{y} \\bigr\\rVert_{A}^{2} $$\n",
    "- Is that all?\n",
    "    - **Nope – trivially solved with $A = 0$**\n",
    "\n",
    "## Learning a metric (2)\n",
    "- Additional constraint: use pairs in $D$ to guarantee non-zero distances\n",
    "$$ \\sum_{\\vec{x},\\,\\vec{y}\\in D}\\bigl\\lVert \\vec{x} - \\vec{y} \\bigr\\rVert_{A} \\ge 1 $$\n",
    "- (choice of 1 is arbitrary; any other constant $c$ would have the effect of replacing $A$ with $c^{2}A$)\n",
    "- Is that all?\n",
    "    - **Nope – need to ensure $A$ is positive semi-definite (why?)**\n",
    "\n",
    "```\n",
    "Why is it important for metric matrices to be positive semi-definite?\n",
    "    a. The matrix can be inverted (raised to -1 power)\n",
    "    b. The matrix is mostly happy\n",
    "    c. The matrix forms a convex basis\n",
    "    d. The eigenvalues of the matrix are real // correct\n",
    "```\n",
    "\n",
    "## Aside!\n",
    "- We used squared Euclidean distance in the first constraint $$ \\min_{A}\\sum_{\\vec{x},\\,\\vec{y}\\in S}\\bigl\\lVert \\vec{x} - \\vec{y} \\bigr\\rVert_{A}^{2} $$\n",
    "- But not in the second! why? $$ \\sum_{\\vec{x},\\,\\vec{y}\\in D}\\bigl\\lVert \\vec{x} - \\vec{y} \\bigr\\rVert_{A} \\ge 1 $$\n",
    "- Squared distance in 2nd constraint would always result in **rank-1 $A$**, i.e. the data would always be projected on a line\n",
    "\n",
    "## Learning a metric (3)\n",
    "- A third constraint: keep $A$ positive semi-definite $$ A \\succeq 0 $$\n",
    "    - (this means the diagonal is always $\\ge 0$)\n",
    "- If $A$ is PSD, its eigenvectors and eigenvalues exist and are real: $$ A = X \\Lambda X^{T} $$\n",
    "- Set any negative eigenvalues to 0: $$ \\Lambda' \\;=\\;\\mathrm{diag}\\bigl(\\max\\{0,\\lambda_{1}\\},\\;\\dots,\\;\\{0,\\lambda_{n}\\}\\bigr) $$\n",
    "- Compute $A’$: $$ A' = X \\Lambda' X^{T} $$\n",
    "\n",
    "## Learning a Metric\n",
    "- We have our constraints!\n",
    "$$ \\min_{A}\\sum_{\\vec{x},\\,\\vec{y}\\in S}\\bigl\\lVert \\vec{x} - \\vec{y} \\bigr\\rVert_{A}^{2} $$\n",
    "$$ \\sum_{\\vec{x},\\,\\vec{y}\\in D}\\bigl\\lVert \\vec{x} - \\vec{y} \\bigr\\rVert_{A} \\ge 1 $$\n",
    "$$ A \\succeq 0 $$\n",
    "- How do we learn $A$?\n",
    "    - *(Hint)* Linear in parameters of $A$\n",
    "    - *(HINT)* First two constraints are verifiably convex\n",
    "\n",
    "## Convex optimization\n",
    "- For diagonal A, this is easy: $$ g(A) \\;=\\; g(A_{11},\\dots,A_{nn})\\;=\\;\\sum_{(x_i,x_j)\\in S}\\|x_i - x_j\\|_{A}^{2}\\;-\\;\\log\\!\\Bigl(\\sum_{(x_i,x_j)\\in \\mathcal{D}}\\|x_i - x_j\\|_{A}\\Bigr) $$\n",
    "- (just a fancy reformulation of the original constraints)\n",
    "- Minimizing $g$ is equivalent to solving original problem, up to multiplication of $A$ by a positive constant\n",
    "- **Gradient descent!** (step-size intrinsically enforces PSD of $A$)\n",
    "\n",
    "- Trickier for full $A$\n",
    "- Gradient ascent + iterative projections\n",
    "- For this to work, constraints needed to be reversed\n",
    "\n",
    "- **Iterate**\n",
    "    - **Iterate**\n",
    "        - $ A \\;\\coloneqq\\; \\arg\\min_{A'}\\;\\{\\,\\|A' - A\\|_{F}\\;:\\;A' \\in C_{1}\\} $\n",
    "        - $ A \\;\\coloneqq\\; \\arg\\min_{A'}\\;\\{\\,\\|A' - A\\|_{F}\\;:\\;A' \\in C_{2}\\} $\n",
    "    - **until** $A$ converges\n",
    "    - $ A \\;\\coloneqq\\; A \\;+\\;\\alpha\\;\\bigl(\\nabla_{A}g(A)\\bigr)_{\\perp \\nabla_{A}f} $\n",
    "- **until** convergence\n",
    "\n",
    "## Constraint reforumulation\n",
    "| Previous                                                                                     | Current                                                                                     | Symbolically |\n",
    "|----------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|--------------|\n",
    "| $$ \\min_{A}\\sum_{\\vec{x},\\,\\vec{y}\\in S}\\bigl\\lVert \\vec{x} - \\vec{y} \\bigr\\rVert_{A}^{2} $$ | $$ \\sum_{\\vec{x},\\,\\vec{y}\\in S}\\bigl\\lVert \\vec{x} - \\vec{y} \\bigr\\rVert_{A}^{2}  \\le 1 $$ | $$ C_1 $$    |\n",
    "| $$ \\sum_{\\vec{x},\\,\\vec{y}\\in D}\\bigl\\lVert \\vec{x} - \\vec{y} \\bigr\\rVert_{A} \\ge 1 $$       | $$ \\max_{A}\\sum_{\\vec{x},\\,\\vec{y}\\in D}\\bigl\\lVert \\vec{x} - \\vec{y} \\bigr\\rVert_{A} $$    | $$ g(A) $$   |\n",
    "| $$ A \\succeq 0 $$                                                                            | $$ A \\succeq 0 $$                                                                           | $$ C_2 $$    |\n",
    "\n",
    "## GA + IP\n",
    "- Previous algorithm:\n",
    "    - **Iterate**\n",
    "        - **Iterate**\n",
    "            - $ A \\;\\coloneqq\\; \\arg\\min_{A'}\\;\\{\\,\\|A' - A\\|_{F}\\;:\\;A' \\in C_{1}\\} $\n",
    "            - $ A \\;\\coloneqq\\; \\arg\\min_{A'}\\;\\{\\,\\|A' - A\\|_{F}\\;:\\;A' \\in C_{2}\\} $\n",
    "        - **until** $A$ converges\n",
    "        - $ A \\;\\coloneqq\\; A \\;+\\;\\alpha\\;\\bigl(\\nabla_{A}g(A)\\bigr)_{\\perp \\nabla_{A}f} $\n",
    "    - **until** convergence\n",
    "- Reformulated algorithm:\n",
    "    - **Iterate**\n",
    "        - **Iterate**\n",
    "            - $ \\sum_{\\vec{x},\\,\\vec{y}\\in S}\\bigl\\lVert \\vec{x} - \\vec{y} \\bigr\\rVert_{A}^{2}  \\le 1 $\n",
    "            - $ A \\succeq 0 $\n",
    "        - **until** $A$ converges\n",
    "        - $ A \\coloneqq A + \\alpha\\nabla\\max_{A}\\sum_{\\vec{x},\\,\\vec{y}\\in D}\\bigl\\lVert \\vec{x} - \\vec{y} \\bigr\\rVert_{A} $\n",
    "    - **until convergence**\n",
    "\n",
    "![ real‐world proof-of-concept of “GA + IP” on a 78-dimensional chem-property dataset. shows the six successive snapshots of the learned full metric matrix and the corresponding histogram of pairwise distances, taken at a few outer-loop iterations of the gradient-ascent + iterative-projection routine](./pics/gaIP_visual.png)\n",
    "- the heatmap of the metric after the 1st-6th outer iterations (inner projections have already converged at each step) are shown, together with the distribution of distances those metrics induce\n",
    "- You can see the matrix slowly morph from its initialization toward a final PSD shape that spreads dissimilar feature-pairs apart (brighter off-diagonals in the heatmap) while the histogram of learned distances shifts and sharpens\n",
    "\n",
    "## Experiments\n",
    "- Generated artificial 3D data<br>\n",
    "![visual of the four toy 3-D point clouds that were fed into the metric learner: 2-class data seperated by y-axis, 2-class data seperated by z-axis, 3-class data seperated by y-axis, and 3-class data seperated by z-axis](./pics/experiments_visual.png)\n",
    "    - 2 class\n",
    "    - 3 class\n",
    "    - separated by y-axis\n",
    "    - separated by z-axis\n",
    "\n",
    "![side-by-side look at (a) original, (b) Newton-optimized (rotated/reshaped) and (c) IP-optimized projections (classes have been collapsed onto a nearly one-dimensional line) of the 2-class data under their learned metrics](./pics/2classExp_visual.png)\n",
    "\n",
    "![side-by-side look at (a) original, (b) Newton-optimized (rotated/reshaped) and (c) IP-optimized projections (classes have been collapsed onto a nearly one-dimensional line) of the 3-class data under their learned metrics](./pics/3classExp_visual.png)\n",
    "\n",
    "## Other Applications\n",
    "- Video scene segmentation\n",
    "- Identifying dynamic textures in videos\n",
    "\n",
    "![visual showing how the same metric‐learning machinery can be dropped into a video‐scene‐segmentation pipeline to pull out “dynamic textures” (grass waving, fire flickering, rippling water)](./pics/otherApps_visual.png)\n",
    "\n",
    "## Other formulations\n",
    "- **Deep metric learning**\n",
    "    - Differentiates different metric constraints\n",
    "        - Contrastive <br> ![Contrastive‐pair losses visual showing pairs (x1, x2), (x3, x4), (x5, x6) connected by links](./pics/constrastivePair_visual.png)\n",
    "            - Pick pairs labeled \"same\" or \"different\"; Penalize large distances for “same” pairs and small distances for “different” pairs\n",
    "        - Triplet <br> ![Triplet losses visual showing (x1, x2, x3) and (x4, x5, x6) linked as well as the ring diagram before and after the deep network](./pics/tripletLosses_visual.png)\n",
    "            - Form triplets **(anchor,positive,negative)** (the ring diagram: anchor in the center, positive inside the margin and negative outside); Enforce $d(\\text{anchor, positive}) + \\alpha < d(\\text{anchor, negative})$ so positives move closer and negatives further away\n",
    "        - Lifted structure <br>![visual showing x1-x6 with many links between them as well as a diagram of the lifted structure architecure: the batch --> cnn --> lifted struct loss](./pics/liftedStructure_visual.png)\n",
    "            - Within each mini-batch of labeled examples, consider all positive and negative pairs at once; Feed the batch through a CNN to get embeddings, then apply a single “lifted” loss that simultaneously pulls together every positive pair and pushes apart every negative pair\n",
    "- **Adaptive densities** <br> ![visual 1: many overlapping class-conditional “balls” of points in an embedding space; after magnet‐loss training, each class’s cluster has been pulled tightly together and pushed far from its neighbors. visual 2: Triplet loss embeddings, magnet loss embeddings, and softmax embeddings are compared for  two example object categories. visual 3: Plots training error versus number of iterations on four benchmarks (Pets, Flowers, Dogs, ImageNet Attributes); In every case the blue magnet‐loss curve converges faster and to a lower error than the orange triplet‐loss curve](./pics/magnetLoss_visual.png)\n",
    "    - Introduces “magnet loss” *(how does it work?)*\n",
    "        - Optimizes over entire neighborhoods simultaneously\n",
    "        - Reduces distribution overlap, rather than just pairs or triplets\n",
    "    - Requires ground-truth labels\n",
    "- **Larg-scale metric learning**\n",
    "    - If feature space is extremely large, iterative eigen-decompositions are a deal-breaker\n",
    "    - Nested convex optimization is a deal-breaker\n",
    "    - Represent metric $A = L^{T}L$\n",
    "        -  Learn $L$ directly, instead of $A$\n",
    "$$ \\min_{L} \\sum_{(x,y)\\in \\mathcal{S}} \\bigl\\lVert L\\,(x - y)\\bigr\\rVert^{2} \\quad \\text{such that} \\quad \\|L(x - y)\\|^{2} \\;\\ge\\; 1,\\quad\\forall\\,(x,y)\\in \\mathcal{D} $$\n",
    "    -  Use hinge loss to induce unconstrained optimization\n",
    "$$ \\min_{L}\\;\\sum_{(x,y)\\in \\mathcal{S}}\\|L(x - y)\\|^{2}\\;+\\;\\lambda\\sum_{(x,y)\\in \\mathcal{D}}\\xi_{x,y} \\quad \\text{such that} \\quad \\|L(x - y)\\|^{2} \\;\\ge\\; 1 - \\xi_{x,y},\\quad\\xi_{x,y} \\;\\ge\\; 0,\\quad\\forall\\,(x,y)\\in \\mathcal{D} $$\n",
    "$$ \\min_{L}\\;\\sum_{(x,y)\\in \\mathcal{S}}\\|L(x - y)\\|^{2}\\;+\\;\\lambda\\sum_{(x,y)\\in \\mathcal{D}}\\max\\bigl(0,\\;1 - \\|L(x - y)\\|^{2}\\bigr) $$\n",
    "    -  Parameter server for SGD-based metric updates\n",
    "\n",
    "![diagram showing the parameter‐server backbone for doing large‐scale, SGD-based metric updates. includes the conceptual flow of L and the nuts-and-bolts implementation of the parameter server](./pics/parameterServer_visual.png)\n",
    "- Conceptual flow of $L$:\n",
    "    - A central server holds the “master” copy of the factor $L$.\n",
    "    - It broadcasts $L$ out to each of several workers.\n",
    "    - Each worker uses its local shard of $\\mathcal{S}$ and $\\mathcal{D}$ pairs to compute an update $\\Delta L_i$, then pushes $\\Delta L_i$ back to the server.\n",
    "    - The server aggregates those $\\Delta L_i$s into a new global $L$, and the loop continues asynchronously\n",
    "- nuts-and-bolts implementation of that parameter server:\n",
    "    - On the server side you have a “Communication Thread” that reads incoming gradient‐messages (IMQ), an “Update Thread” that applies them to the Global Parameter, and a queue for outgoing new parameters (OMQ).\n",
    "    - On each worker, a symmetric pair of threads maintains a Local Copy of $L$: one thread pulls new parameters down from OMQ, another thread pulls training data, computes its local gradient, and pushes $\\Delta L$ back up via IMQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4ed214-542f-450d-b1c7-a0a360e3aafb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
