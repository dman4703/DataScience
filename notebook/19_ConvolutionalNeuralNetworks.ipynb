{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a6f3141-2bdf-4683-b02a-1d29fea69f45",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "## [The Neural Network Zoo](http://www.asimovinstitute.org/neural-network-zoo/)\n",
    "<img \n",
    "    src=\"./pics/nn_zoo.png\"\n",
    "    alt=\"infographic displaying various neural networks: perceptron, feed foward, radial basis network, deep feed forward, recurrent neural network, long/short term memory, gated recurrent unit, auto encoder, variational AE, denoising AE, sparse AE, markov chain, hopfield network, boltzman machine, restricted BM, deep belief network, deep convolutional network, deconvolutional network, deep convolutional inverse graphics network, generative adversial network, liquid state machine, extreme learning machine, echo state network, deep residual network, differentiable neural computer, neural turing machine, capsule network, kohonen network, attention network. Neuron. Nodes are also labeled: input cell, backfed input cell, noisy input cell, hidden cell, probablistic hidden cell, spiking hidden cell, capsul cell, output cell, match input output cell, recurrent cell, memory cell, gated memory cell, kernel, convolution/pool\"\n",
    "    style=\"width:50%;\"/>\n",
    "- CNNs: Deep Convolutional Network, Deconvolutional network, deep convolutional inverse graphics network\n",
    "\n",
    "## Convolution\n",
    "- Basically a fancy way of saying “multiplication”\n",
    "- Originally devised to make non-differentiable signals differentiable\n",
    "- KDE is related to convolution\n",
    "- For an input function $f$ and convolutional filter $g$:\n",
    "\n",
    "```\n",
    "scipy.signal.convolve(in1, in2, mode ='full', method='auto')\n",
    "    Convolve two N-dimensional arrays.\n",
    "    Convolve 'in1' and 'in2', with output size determined by 'mode' argument.\n",
    "    Parameters: in1: array_like\n",
    "                    First input\n",
    "                in2: array_like\n",
    "                    Second input. Should have the same number of dimensions as in1.\n",
    "                mode: str{'full', 'valid', 'same'}, optional\n",
    "                    A string indicating the size of the output:\n",
    "```\n",
    "\n",
    "![three plots demonstrating the plot of convolution. Original pulse: a rectangular (step) signal. Filter impulse response: a smooth kernel (e.g. a little bump or Gaussian). Filtered signal: the result of convolving the pulse with the kernel, which smooths out the sharp edges into a gently rising and falling waveform.  illustrates how convolution “blends” an input with a kernel to produce a smoothed (and differentiable) output](./pics/convolution_ex.png)\n",
    "\n",
    "```\n",
    "What is the infinite verb form of \"convolution\"?\n",
    "    a. convolve // correct\n",
    "    b. convolute\n",
    "    c. convolutionize\n",
    "    d. convolvar\n",
    "```\n",
    "\n",
    "- Can be viewed as an *integral transform*\n",
    "    - One of the signals is shifted\n",
    "$$ (f \\circledast g)(t) = \\int_{-\\infty}^{\\infty}f(\\tau)g(t - \\tau)d\\tau = \\int_{-\\infty}^{\\infty}f(t - \\tau)g(\\tau)d\\tau $$\n",
    "\n",
    "![plot (x-axis: t and \\tau) showing f(\\tau) (a square wave centered at 0, height of 1, width 1), g(t-\\tau) (a square wave of height of 1, width 1), area under f(\\tau)g(t-\\tau), and (f \\circledast g)(t). As g(t-\\tau) moves from right to left, the area is highlighted and (f \\circledast g)(t) is drawn. (f \\circledast g)(t) looks like a triangle of width 2 (centered around 0) and height 1](./pics/conv_integralTransform1.gif)\n",
    "![plot (x-axis: t and \\tau) showing f(\\tau) (a scausal exponential‐decay signal starting as 1 at t=0), g(t-\\tau) (a square wave of height of 1, width 1), area under f(\\tau)g(t-\\tau), and (f \\circledast g)(t). As g(t-\\tau) moves from right to left, the area is highlighted and (f \\circledast g)(t) is drawn. (f \\circledast g)(t) gradually increases starting at -0.5, before starting to decay at 0.5 and following the curve of f(\\tau)](./pics/conv_integralTransform2.gif)\n",
    "\n",
    "## Convolution in 2D\n",
    "- 2D convolutions are critical in computer vision\n",
    "- Basic idea is still the same\n",
    "    - Choose a kernel\n",
    "    - Run kernel over image\n",
    "    - Build a representation of the convolved image (likely an intermediate representation)\n",
    "- Lots of applications\n",
    "\n",
    "![example of a 2D discrete convolution on an image. Left is a 5x5 binary \"image\". An orange 3x3 patch of the current receptive field is highlighted. In the orange patch, each cell has a red x1/x0 label. Each image pixel is multiplied by its corresponding weight, then summed. This sum becomes the corresponding entry in the convolved feature map (shown on the right)](./pics/convolution2D_ex.gif)\n",
    "\n",
    "- Specific kernels can highlight different image features\n",
    "- This kernel is an edge detector (others can be smoothers, sharpeners, etc) <br> ![visual of a 2D convolution used for edge detection in computer vision. Left: the original input image (a small animal’s head). Center: the convolution kernel (3x3 matrix with 8 in the center and -1 in all other spots) which is a discrete Laplacian filter (it accentuates regions of rapid intensity change). Right: the resulting feature map, where the filter has highlighted the image’s edges and contours](./pics/convKernel_visual.png)\n",
    "\n",
    "- Works basically the same as 1D\n",
    "- Filter / kernel computes a dot product with underlying pixels\n",
    "- Generates an output\n",
    "- Shift kernel and repeat\n",
    "\n",
    "![schematic of a single step in a 2D discrete convolution. Left (“kernel”): the small filter (e.g. a 3×3 weight matrix). Middle (“input”): the larger image, with the current receptive field (the patch under the kernel) highlighted in blue. Right (“output”): the feature map, where that one patch’s weighted sum (the dot-product of kernel & patch) is written into the corresponding output pixel (shown in red).](./pics/convolution2D_ex.png)\n",
    "\n",
    "![diagram walking through one step of a 2D convolution using the Sobel Gx edge-detection kernel. Left: A patch of the input image is highlighted, with the “source pixel” at its center. Middle: The 3×3 Sobel Gx filter is overlaid on that patch. Right: Each of the nine image values is multiplied by its corresponding kernel weight, and those products are summed. ](./pics/convolution2D_ex2.png)\n",
    "\n",
    "- **Stride** dictates how far the kernel moves after each convolution\n",
    "- **Padding** is used to help with edge cases\n",
    "- Pictured: stride of 2, padding of 1 <br> ![visual of how padding and stride works in 2D convolution. padding=1: add a one-pixel “frame” of zeros (or whatever pad value) around the original input, so the kernel can still be centered on the very edge pixels. stride=2: Instead of sliding the filter one pixel at a time, you move it two pixels over (and down) between applications. output sampling: Each time the filter (the solid red box) is applied, you compute a single dot-product and write it into the corresponding cell of the (smaller) output grid. Because of stride-2, the output is downsampled by a factor of two in both dimensions.](./pics/stridePadding_ex.png)\n",
    "\n",
    "- Repeated convolutions can generate large intermediate feature maps\n",
    "- “Pooling” is used to reduce dimensionality of feature maps while maintaining most informative features\n",
    "- Mean-pooling, **max-pooling**\n",
    "- Functions as a regularizer (or an infinitely-strong prior)\n",
    "\n",
    "![visual illustrating a pooling step (specifically max-pooling) that follows convolution. Left: a (very) large intermediate feature map produced by repeated convolutions. Right: the much smaller “pooled” feature map.](./pics/pooling_visual.gif)\n",
    "\n",
    "![diagram of a single depth slice on the left (4x4 matrix, with 4 2x2 portions color coded) and the pooled feature map on the left (2x2, each square is color coded to match the corresponding 2x2 portion) after a max pool with 2x2 filters and stride 2](./pics/pooling_visual.png)\n",
    "\n",
    "![diagram illustrating how max-pooling produces a translation-invariant response. Two panes showing how a 5 in different orientations is processed. each '5' goes through three local feature detectors; on the left they all fire most strongly on the centered “5,” on the right they shift firing to the rightmost detector when the “5” moves. the pooling unit then takes the maximum of the three input feature detectors, in both cases it still outputs a large response](./pics/maxPooling_ex.png)\n",
    "\n",
    "## Filters\n",
    "- Different filter topologies\n",
    "- Captures long-range pixel dependencies\n",
    "- *Very* computationally expensive to implement\n",
    "\n",
    "![visual showing dilated filters at D=1, 2, 3. D = 1: a standard dense 3x3 kernel. D = 2: a 3x3 kernel whose taps are spaced two pixels apart, covering a 5x5 region with only 3×3 weights. D = 3: taps spaced three pixels apart, covering a 7x7 region with 3x3 weights.](./pics/filterDilation_visual.png)\n",
    "\n",
    "## Convolution\n",
    "- Key point: **parameter sharing**\n",
    "- Images are sparse\n",
    "    - Pixel dependencies don’t span arbitrarily large distances\n",
    "    - Important effects are local\n",
    "- Instead of a fully-connected network we have one that is more sparsely-connected\n",
    "\n",
    "![ example of a convolutional feature map. left is a grayscale image of a dog. right is the output when using an edge detecting kernel. shows how one small filter shared across every location produces a sparse, parameter-efficient representation highlighting the image’s local structure](./pics/paramSharing_visual.png)\n",
    "\n",
    "![connectivity graph of a 1D convolutional layer laid out as a little neural net. Bottom row (x1 to x5) are input pixels. Top row (s1 to s5) are convolved outputs. each s_i pulls from all x parameters](./pics/paramSharing_nn.png)\n",
    "\n",
    "## Parameter Sharing\n",
    "![visual contrasting a naive fully-connected layer with a convolution-style (locally-connected) layer. On a 1000x1000 image, fully connected NN will have 10^12 parameters. Locally connected NN (filter size 10x10) will only have 100M parameters.](./pics/paramSharing_ex.png)\n",
    "\n",
    "## CNNs in practice\n",
    "- Stacked\n",
    "    - Convolutions\n",
    "    - Pools\n",
    "    - Activations\n",
    "- Fully-connected classification layer <br> ![example of the stacked-layer structure of a CNN: Input volume ->Conv layer (180 weights + 5 biases) -> maxpool -> nonlinearity (like ReLU) -> conv layer (450 weights + 10 biases) -> nonlinearity -> flatten -> fully connected layer (1 600 weights + 10 biases) -> nonlinearyity -> output (10-way score vector (one per class, here 0–9 for digit classification))](./pics/cnn_struct.png)\n",
    "\n",
    "- Pattern can be repeated several times <br> ![visual showing proccess: 128x128x3 img -> 7x7 conv layer (out 32) pad 3 stride 1 -> relu -> pool 2x2 pad 0 stride 2 -> repeate conv + relu + pool two more times -> FC layers out-3 -> OUT-Scores](./pics/cnn_patternRepeat.png)\n",
    "- Still “deep”, but convolutions are **the most important part**\n",
    "\n",
    "- Filters are the things that “search” for something in particular in an image\n",
    "- To search for many different things, have many different filters\n",
    "\n",
    "![visual showing a filter scanning over an input to produce two feature maps](./pics/filter_search.gif)\n",
    "![visual showing dimensionality change when you apply a single convolutional layer to an input. Left: the input volume is 32x32 pixels with 3 channels. Right: after convolving with 6 filters, you get 6 activation maps, each of spatial size 28x28.](./pics/diffFilters_visual.png)\n",
    "\n",
    "- Hyperparameters relevant to CNNs:\n",
    "    - Kernel size\n",
    "        - Usually small\n",
    "    - Stride\n",
    "        - Usually 1 (larger for pooling layers)\n",
    "    - Zero padding depth\n",
    "        - Enough to permit convolutional output size to be the same as input size\n",
    "    - Number of convolutional filters\n",
    "        - Number of “patterns” for the network to search for\n",
    "\n",
    "- 1$\\times$1 convolutions are a special case\n",
    "- Convolve the **feature maps**, rather than the **pixel maps**\n",
    "- Function as a dimensionality reduction step (like pooling)\n",
    "    - Can also be used in pooling\n",
    "\n",
    "![visual illustrating a 1×1 convolution: instead of sliding a 3×3 (or larger) spatial window, your filter is just a single-pixel “column” that spans all input channels. At each (x,y) location you take that 1×1×D patch (blue) and dot it with your 1×1×D weights, producing one output value (teal) at the same (x,y). In effect, a 1×1 conv mixes information across channels (and can reduce or expand depth) without touching neighboring pixels in space.](./pics/1x1Conv_visual.gif)\n",
    "\n",
    "## CNN Applications: Object Localization\n",
    "- Two discrete steps:\n",
    "    - Localizing a bounding box (*regression*)\n",
    "    - Identifying the object (*classification*)\n",
    "\n",
    "![architecture for a CNN-based object localization/detection model. img -> convolution and pooling -> final conv feature map -> two parallel \"heads\" branching off the feature map: A classification head (fully-connected layers -> class scores) and A regression head (fully-connected layers -> bounding-box coordinates). Together, these allow the network both to identify what is in the image and where it sits](./pics/onjLocal_arch.png)\n",
    "\n",
    "- Generate “region proposals” <br> ![visualization of region proposals in an object-detection pipeline: a dense set of candidate bounding-boxes (at different positions, scales, and aspect ratios) that the detector will later classify and refine to find the actual object’s location](./pics/objLocal_retionalprop.png)\n",
    "- Classification accuracy\n",
    "\n",
    "|                                       | R-CNN        | Fast R-CNN      | Faster R-CNN     |\n",
    "|---------------------------------------|--------------|-----------------|------------------|\n",
    "| Test time per image (with proposals)  | 50 seconds   | 2 seconds       | **0.2 seconds**  |\n",
    "| (Speedup)                             | 1x           | 25x             | **250x**         |\n",
    "| mAP (VOC 2007)                        | 66.0         | **66.9**        | **66.9**         |\n",
    "- The best result now is Faster RCNN with a resnet 101 layer\n",
    "\n",
    "## CNN Applications: Single-shot Detection\n",
    "- Combines region-proposal (regression) and object detection (classification) into a single step\n",
    "- Use deep-level feature maps to predict class scores and bounding boxes\n",
    "- Families of Single-shot detectors:\n",
    "    - YOLO (single activation map for both class and region)\n",
    "    - SSD (different activations)\n",
    "    - R-FCN (like Faster R-CNN)\n",
    "\n",
    "![visual showing grid‐cell scheme used in single‐shot object detectors. image is overlaid with a coarse SxS grid (in red). Each grid cell is responsible for predicting object bounding boxes whose centers fall inside it. The green boxes are the actual predicted bounding boxes for the pedestrians, produced directly in one pass—no separate region‐proposal step. shows how a detector like YOLO partitions the image into fixed cells and simultaneously regresses multiple boxes and class scores from those cells](./pics/singleShot_ex.png)\n",
    "\n",
    "## CNN Applications: Object Segmentation\n",
    "- Create a map of the detected object areas\n",
    "- “Fully-convolutional” networks\n",
    "    - Substitute fully-connected layer at end for another convolutional layer\n",
    "    - Activations show object\n",
    "- Resolution is lost in upsampling step\n",
    "    - Skip-connections to bring in some of the “lost” resolution\n",
    "- *EXTREME* Segmentation\n",
    "    - Replace upsampling with a complete deconvolution stack\n",
    "\n",
    "![canonical fully-convolutional network (FCN) architecture for semantic segmentation. 1) A standard convolutional backbone (e.g. VGG) turns the input image into progressively smaller, deeper feature maps. 2) Instead of ending in dense (FC) layers, it uses a final 1×1 convolution to produce an low-resolution “score map” (21 channels here, one per class). 3) That coarse map is then upsampled (with learnable deconvolution/transpose-conv filters) back to the original image size. The result is a pixel-wise prediction map which is compared against the ground-truth segmentation.  shows how you convert a classification CNN into a dense, end-to-end trainable model that labels every pixel.](./pics/objSeg_visual.png)\n",
    "\n",
    "![visual showing how you can turn a standard image classifier into a weak localization model by “convolutionalizing” its fully-connected layers to produce a class activation heatmap. Top row: a CNN + FC layers spits out a single “tabby cat” score—no idea where in the image the cat lives. convolutionalization: you replace those FC layers with equivalent 1×1 convolutions. Bottom row: that same network now outputs a low-resolution spatial map (the “tabby cat heatmap”) that lights up the regions most responsible for the “tabby cat” prediction.](./pics/localizationModel_imageClass.png)\n",
    "\n",
    "![comparison of three fully-convolutional network (FCN) variants for semantic segmentation, showing how adding skip-connections at different depths trades off semantic strength for spatial detail. FCN-32s:  no skips—upsamples the very coarse final feature map by ×32. You get strong, high-level “semantics” but very blocky masks. FCN-16s:  injects a skip from the pool-4 layer (stride-16) before upsampling. You fuse somewhat finer features, so the mask is sharper than FCN-32s. FCN-8s: adds another skip from pool-3 (stride-8) as well. That restores even more spatial detail, producing the most finely resolved segmentation.](./pics/segmentation_comparison.png)\n",
    "\n",
    "- \"DeconvNet\": *Super*-expensive to train <br> ![DeconvNet (encoder–decoder) architecture for pixel-wise semantic segmentation. Convolutional “Encoder”: A standard CNN (e.g. VGG-style) repeatedly applies convolutions + ReLUs and max-pooling, shrinking the spatial size from 224×224 down to a 7×7 “bottleneck.” Deconvolutional “Decoder”: It then mirrors that process: at each decoder stage you unpool (using the saved pooling switches to place activations back where they came from) and follow with learned deconvolution filters to upsample back through 14×14, 28×28, 56×56, 112×112 and finally reconstruct a 224×224 segmentation map. arrows show the skip-connections of pooling indices that guide the unpooling, letting the network recover fine spatial detail (important for small objects), at the cost of a very heavyweight, expensive-to-train model](./pics/deconvnet_arch.png)\n",
    "- But results are excellent\n",
    "    - Particularly for small objects <br> ![comparison of segmentation quality of a vanilla FCN versus a DeconvNet on two example images (ground truth also shown). DeconvNet has much sharper, more precise boundaries and better recovery of fine details](./pics/devconvnet_comparison.png)\n",
    "\n",
    "```\n",
    "The parameter sharing/ receptive field architecture unique to CNNs enables dramatically reduced parameter counts in neural architectures, exploiting the inherent sparsity of images. What is a disadvantage of this approach?\n",
    "    a. CNNs tend to blur object boundaries, much like optical flow.\n",
    "    b. CNNs cannot build up an internal representation of object hierarchy.\n",
    "    c. CNNs struggle to identify objects in high-resolution images.\n",
    "    d. CNNs have no notion of absolute object locality, or of relative positioning of multiple objects. // correct\n",
    "```\n",
    "\n",
    "## Conclusions\n",
    "- CNNs are mostly “convolutions inside a deep network”\n",
    "    - Main operator (i.e. **most important**) is the convolution\n",
    "    - Exploits image sparsity: important features are **local**\n",
    "- A couple new[ish] tricks include\n",
    "    - Automatically learning the filters as part of the training process\n",
    "    - Using pooling\n",
    "    - 1$\\times$1 convolutions\n",
    "- Applications include\n",
    "    - Object detection (is there an object)\n",
    "    - Object localization and segmentation (where is the object)\n",
    "    - Object classification (what is the object)\n",
    "    - Zero- and single-shot detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fcd2b1-18ff-44e9-9f37-93ded12d7a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
