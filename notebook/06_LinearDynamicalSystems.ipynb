{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99b52801-8fb0-4dd5-b264-38069847ebd1",
   "metadata": {},
   "source": [
    "# Linear Dynamical Systems\n",
    "\n",
    "## Last time...\n",
    "- Motion analysis via optical flow\n",
    "- Parametric vs energy-based formulations\n",
    "- Importance of assumptions\n",
    "- Modern formulations\n",
    "    - Robustness to outliers (large optical flow)\n",
    "    - Relatedness to markov random fields\n",
    "    - Coarse-to-fine image pyramids\n",
    "\n",
    "## This lecture\n",
    "- A specific type of motion: **dynamic textures**\n",
    "\n",
    "![Dynamic Textures visual/example](./pics/dynamicTextures_visual.png)\n",
    "\n",
    "## Dynamic Textures\n",
    "- *“Dynamic textured sequences are scenes with complex motion patterns due to interactions between multiple moving components.”*\n",
    "- Examples\n",
    "    - Blowing leaves\n",
    "    - Flickering flames\n",
    "    - Water rippling\n",
    "- **Multiple moving components: problematic for optical flow**\n",
    "- How to analyze dynamic textures?\n",
    "\n",
    "## Dynamical Models\n",
    "- Goal: an effective procedure for tracking changes over sequences of images, while maintaining a certain coherence of motions\n",
    "- Hand tracking<br>\n",
    "![demonstration of using a dynamical (state-space) model to track a deformable contour](./pics/dynamicalModels_hand.png)\n",
    "- Top row: slow movements\n",
    "- Bottom Row: fast movements\n",
    "- Fixed curves or priors cannot exploit coherence of motion\n",
    "\n",
    "## Linear Dynamical Models\n",
    "- Two main components (using notation from Hyndman 2006):\n",
    "- Apperance Model:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_t &= C\\,x_t + u_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- State Model:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_t &= A\\,x_{t-1} + W\\,v_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "## Autoregressive Models\n",
    "- This is the definition of a 1st-order autoregressive (AR) process!\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_t &= A\\,x_{t-1} + W\\,v_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "- Each observation ($x_t$) is a function of previous observations, plus some noise\n",
    "- **Markov model!**\n",
    "- AR models can have higher orders than 1\n",
    "- Each observation is dependent on the previous d observations\n",
    "$$ x_t \\;=\\; A_1\\,x_{t-1} \\;+\\; A_2\\,x_{t-2} \\;+\\;\\dots\\;+\\;A_d\\,x_{t-d} \\;+\\; W\\,v_t $$\n",
    "\n",
    "## Appearance Model\n",
    "- $y_t$: image of height $h$ and width $w$ at time $t$, usually flattened into $1 \\times hw$ vector (Image in a sequence)\n",
    "- $x_t$: state space vector at time $t$, $1 \\times q$ (where $q <<< hw$) (Low-dimensional \"state\") \n",
    "- $u_t$: white Gaussian noise (Noise inherent to the system)\n",
    "- $C$: output matrix, maps between spaces, $hw \\times q$\n",
    "$$ y_t = C_\\,x_t + u_t $$\n",
    "- Each of these is 1 column of $C$\n",
    "- There are $q$ of them (first 4 shown here).\n",
    "\n",
    "![an example observed frame, the first four columns of C (an eigen-appearance image showing one of the dominant spatial patterns)](./pics/appModel_visal.png)\n",
    "- shows how the appearance of the dynamic texture is factored into a small set of basis images (the columns of $C$), with their weights evolving over time according to the dynamical model.\n",
    "- How do we learn the appearance model?\n",
    "- Choose state-space dimension size $q$\n",
    "- Noise term is i.i.d Gaussian\n",
    "$$ Y = \\bigl[\\bar y_{1},\\;\\bar y_{2},\\;\\dots,\\;\\bar y_{f}\\bigr]^{T} $$\n",
    "$$ Y = U\\,\\Sigma\\,V^{T} $$\n",
    "$$ C = \\hat{U} $$\n",
    "- $\\hat{U}$ is a matrix of the first $q$ columns of $U$\n",
    "$$ X \\;=\\; \\hat{\\Sigma}\\,\\hat{V}^{T} $$\n",
    "- $\\hat{V}$ is a matrix of the first $q$ columns of $V$, and $\\hat{\\Sigma}$ is a diagonal matrix of the first $q$ singular values\n",
    "\n",
    "## State Model\n",
    "- $x_t$ and $x_{t-1}$: Low dimensional state space vectors at times $t$ and $t – 1$, each $1 \\times q$ vector\n",
    "- $A$: State transition matrix, $q \\times q$ matrix\n",
    "- $W$: driving noise, $q \\times q$ matrix\n",
    "- $v_t$: white Gaussian noise\n",
    "$$ x_t = A_\\,x_{t-1} + W_\\,v_t $$\n",
    "- Three textures\n",
    "- q = 2\n",
    "\n",
    "![ plot is showing the evolution of the 2-D hidden state x_t (so q=2) for three different dynamic textures](./pics/stateModel_visual.png)\n",
    "\n",
    "## LDS as Generative Models\n",
    "- Once we’ve learned the parameters, we can *generate new instances*<br>\n",
    "![visual showing images generated using LDS for flame and water](./pics/ldsGen_visual.png)\n",
    "- Major strength of LDS!\n",
    "\n",
    "## Problem with LDS\n",
    "- PCA = Linear + Gaussian\n",
    "- What if the *state space* isn’t linear, or data aren’t Gaussian?\n",
    "- Nonlinear appearance models\n",
    "    - Wavelets\n",
    "    - IsoMap\n",
    "    - LLE\n",
    "    - Kernel PCA\n",
    "    - Laplacian Eigenmaps\n",
    "- These introduce their own problems!\n",
    "- Comparing LDS models\n",
    "- Given a sequence $Y$: $ \\theta = (C, A, Q) $\n",
    "- New sequence $Y'$: $ \\theta' = (C', A', Q') $\n",
    "- How do we compare these systems?\n",
    "```\n",
    "if C1 == C2 \\\n",
    "   and A1 == A2 \\\n",
    "   and Q1 == Q2:\n",
    "    ...\n",
    "// WRONG!\n",
    "```\n",
    "- Despite linear formulation, $\\theta$ are NOT Euclidian\n",
    "- Valid distance metrics include spectral methods and distribution comparators\n",
    "\n",
    "## Comparing LDS\n",
    "- Select multiple, non-overlapping patches from each video\n",
    "- Build LDS for each patch\n",
    "\n",
    "![“bag of dynamical systems” pipeline for comparing and quantizing video clips via their LDS models](./pics/comparingLDS_visual.png)\n",
    "- Embed the LDS in low-dimensional space\n",
    "- Compute cluster centroids in embedding space\n",
    "    - These centroids become *codewords*\n",
    "- Represent videos as a *document of codewords*\n",
    "$$p = \\arg\\min_{j} \\|\\,e_{j} - k_{i}\\|^{2}$$\n",
    "    - Compute TF-IDF\n",
    "$$ w_{ik} \\;=\\; \\frac{N_{ki}}{N_{i}}\\;\\ln\\!\\Bigl(\\frac{V}{V_{i}}\\Bigr) $$\n",
    "- Perform classification on document weight vectors\n",
    "\n",
    "![confusion matrix for the dynamic‐texture classification experiment (eight classes: Boiling, Fire, Flowers, Fountain, Sea, Smoke, Water, Waterfall)](./pics/comparingLDS_confMatrix.png)\n",
    "- Rows correspond to the true class of each test video patch,\n",
    "- Columns to the predicted class,\n",
    "- Cell values are the fraction (or percentage) of patches of the true class (row) that were assigned to each predicted class (column).\n",
    "- The dark diagonal entries (all 1.0 or near it) show high correct‐classification rates for most textures, while the off‐diagonals (shaded gray) reveal the main confusions—for example, Fountain is misclassified as Waterfall about 48% of the time, and Water is sometimes confused with Boiling (~46%).\n",
    "\n",
    "## Deep learning + dynamic textures\n",
    "\n",
    "![deep‐learning approach to dynamic‐texture classification](./pics/deepLearning_approach.png)\n",
    "- Model Architecture\n",
    "    - take each video and slice it along three orthogonal planes: $xy$ (purely spatial), $xt$ (horiztonal time), $yt$ (vertical time)\n",
    "    - You feed each slice‐stack into its own copy of a pre‐trained “Texture CNN,” fine‐tuning each stream separately via backpropagation\n",
    "    - Each stream produces per‐class scores, which you can fuse or vote on at test time\n",
    "- Classification Results\n",
    "    -  A confusion matrix over eight dynamic‐texture classes (“boil,” “fire,” “flower,” “fountain,” “sea,” “smoke,” “water,” “waterfall”)\n",
    "    -  Entries on the diagonal (in black) are high (near 1.0), showing strong per‐class accuracy; off‐diagonal gray cells show the few remaining confusions.\n",
    "- Example Slices\n",
    "    - Three example sequences (foliage, traffic, sea) from the DynTex database.\n",
    "    - For each, you see the $xy$ slice (spatial appearance), the $xt$ slice (horizontal motion over time), and the $yt$ slice (vertical motion over time).\n",
    "    - These illustrate why you need both spatial and temporal cues to recognize each dynamic texture.\n",
    "- combining three fine‐tuned CNN streams—each looking at a different spatiotemporal view—yields high accuracy on dynamic‐texture classification.\n",
    "\n",
    "## Mamba state space models\n",
    "- Motivated by performance and space considerations of Transformer architectures in large language models\n",
    "- (Conceptual) combination of RNN + CNN + AR models\n",
    "- Innovations\n",
    "    - Input selection mechanism\n",
    "    - Hardware-aware algorithm\n",
    "    - Architecture\n",
    "- Vision Mamba (ViM) for image processing\n",
    "- Upshot: beat Transformer models of same parameter size, equivalent performance to Transformer models of 2x parameter size\n",
    "- Much faster inference across the board (ie, generation) and with fewer resources\n",
    "![report on how the new “Mamba” state‐space models compare to standard Vision Transformers (DeiT-Ti) of similar or larger size, across accuracy, speed, and memory](./pics/mambaStateSpaceModel_plots.png)\n",
    "## Conclusion\n",
    "- Dynamic textures are motion with statistical regularity\n",
    "- Regularity can be exploited through parametric representation\n",
    "- Linear dynamical systems (LDS)\n",
    "    - Autoregressive models (AR)\n",
    "    - Markov assumption\n",
    "    - Representation model + State model\n",
    "    - Generative Models\n",
    "- Deep networks can learn the same feature set and in some cases exceed the performance of LDS (though are harder to train)\n",
    "- Mamba state space models make LDS-like architectures cool again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f266dcfa-1f98-4008-a972-6d22d7ac8ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
