{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33657930-3eb9-4ff9-b0d8-b92ff9919091",
   "metadata": {},
   "source": [
    "# Generative AI\n",
    "\n",
    "## [The Neural Network Zoo](http://www.asimovinstitute.org/neural-network-zoo/)\n",
    "<img \n",
    "    src=\"./pics/nn_zoo.png\"\n",
    "    alt=\"infographic displaying various neural networks: perceptron, feed foward, radial basis network, deep feed forward, recurrent neural network, long/short term memory, gated recurrent unit, auto encoder, variational AE, denoising AE, sparse AE, markov chain, hopfield network, boltzman machine, restricted BM, deep belief network, deep convolutional network, deconvolutional network, deep convolutional inverse graphics network, generative adversial network, liquid state machine, extreme learning machine, echo state network, deep residual network, differentiable neural computer, neural turing machine, capsule network, kohonen network, attention network. Neuron Nodes are also labeled: input cell, backfed input cell, noisy input cell, hidden cell, probablistic hidden cell, spiking hidden cell, capsul cell, output cell, match input output cell, recurrent cell, memory cell, gated memory cell, kernel, convolution/pool\"\n",
    "    style=\"width:50%;\"/>\n",
    "- Gen AI: generative adversial network, liquid state machine, extreme learning machine, echo state network\n",
    "\n",
    "## Caveat Empire\n",
    "![meme reminding that ML systems merely mimic patterns without true comprehension by comparing ML algos to a parrot. Like a parrot, ML algos: learn random phrases, doesnt understant what it learns, occasionally speaks nonsense. However, while the parrot is a cute birdie, the ML algo is not](./pics/ml_caveat.png)\n",
    "\n",
    "## What is a \"generative model\"?\n",
    "![visual contrastinc discriminative and generative modeling approaches. Discriminative focuses on learning the boundary that separates class labels directly, i.e. modeling P(Y|X). Generative Learns the full data distribution for each class, modeling P(X, Y) and P(Y), thus can both classify and generate new samples](./pics/discrimGen_contrast.png)\n",
    "- Discriminative\n",
    "    - Logistic regression\n",
    "    - Support vector machines\n",
    "    - Random forests\n",
    "    - $P(Y \\mid X)$\n",
    "- Generative\n",
    "    - Gaussian Naive Bayes\n",
    "    - Variational Autoencoders\n",
    "    - Adversarial networks\n",
    "    - $P(X, Y)$ and $P(Y)$\n",
    "\n",
    "## Generative Models\n",
    "![visual of the generative approach. Right panel (true data distribution): The blue shape shows the true support of the data, p(x), in the high-dimensional “image space,” with black dots marking actual samples. Left panel (model’s generated distribution): The green shape is the model’s estimate \\hat{p}(x), i.e. the distribution from which it can draw new samples. Loss arrow: The red, dashed arrow indicates that learning proceeds by minimizing some divergence or loss between \\hat{p}(x) and the true p(x).](./pics/genModeling_visual.png)\n",
    "\n",
    "![taxonomy of deep generative modeling approaches. Level order traversal: {Maximum Likelihood, Explicit density, implicit density, traceable density (fully visible belief nets, NADE, MADE, PixelRNN, change of variables models - nonlinear ICA), approximate density, markov chain (GSN), Direct (GAN), null, null, variational (variational autoencoder), markov chain (Boltzmann machine), null, null, null, null}](./pics/genModeling_taxonomy.png)\n",
    "\n",
    "- Learning a *distribution* or *manifold*\n",
    "    - Statistical notion of *how the data were generated*\n",
    "- $P(X)$ asks: how *likely* is the data point $X$?\n",
    "    - If likely $\\rightarrow$ $X$ was **generated** by this process\n",
    "- Compare to $P(Y)$, which asks: how likely is the *label*?\n",
    "    - If likely $\\rightarrow$ $X$ has label $Y$\n",
    "\n",
    "![Top panel: denoising autoencoder learns a vector field that “flows” noisy points back onto the true data manifold; the black curve is the manifold, red ×’s are clean samples, the gray circle marks where corruptions occur, and green arrows show the denoising vectors that point corrupted inputs inward—approximating the gradient of the data density. Bottom panel: Each small circle is a single face image; Arrows connect each face to its nearest neighbors in latent‐space, forming a tangled “sheet” that encodes smooth changes in pose/expression; Together, they show that nearby points on the manifold correspond to visually similar faces.](./pics/genModeling_learning.png)\n",
    "\n",
    "## Types of Generative Models\n",
    "\n",
    "### Probabilistic Graphical Models\n",
    "- Arrows represent conditional dependencies between random variables\n",
    "$$ P(X_{1}, \\ldots, X_{n}) = \\prod_{i=1}^{n}P(X_{i} \\mid \\text{parents}_{i}) $$\n",
    "$$ P(A, B, C, D) = P(A)P(B)P(C,D \\mid A,B) $$\n",
    "- Structure is used in generative models\n",
    "    - Laten generating distribution (hidden)\n",
    "    - Observed variables (influenced by laten vars)\n",
    "\n",
    "![directed graphical model (a “Bayesian-network-style” PGM) over four random variables A, B, C, D. Each circle is a variable, and each arrow X -> Y means \"Y is conditionally dependent on X.\" A and B are roots. A and B point to D. B also points to C. C and D point to each other, indicating a mutual (feedback) dependency. Altogether this graph encodes the factorization P(A, B, C, D) = P(A)P(B)P(C,D \\mid A,B) via its structure.](./pics/probGraphicModel_visual.png)\n",
    "\n",
    "### Variational Inference\n",
    "- What is variational inference?\n",
    "- Good for learning latent variable models (i.e., generating distributions of data)\n",
    "- For each observation $x$ we assign a hidden variable $z$; our model $p$ describes the joint distribution between $x$ and $z$\n",
    "\n",
    "- $p_{\\theta}(z)$ is very easy\n",
    "- $p_{\\theta}(x \\mid z)$ is easy\n",
    "- $p_{\\theta}(x, z)$ is easy\n",
    "- $p_{\\theta}(x)$ is super-hard\n",
    "- $p_{\\theta}(z \\mid x)$ is mega hard\n",
    "- Of course, $p_{\\theta}(x)$ and $p_{\\theta}(z \\mid x)$ are the things we want to calculate\n",
    "    - Inference is $p(z \\mid x)$\n",
    "    - Learning involves $p(x)$\n",
    "\n",
    "- Rather than learning $p(z \\mid x)$ directly, variational inference approximates with $q(z \\mid x)$\n",
    "- Maximize the evidence lower bound (ELBO): $$ \\text{ELBO}(\\theta, \\psi) = \\sum_{n}\\log{p(x_{n})} - \\text{KL}[q_{\\psi}(z \\mid x_{n}) || p_{\\theta}(z \\mid x_{n})] $$\n",
    "\n",
    "### Recall: Autoencoders\n",
    "![diagram illustrating a convolutional autoencoder workflow. Input image is fed into an Encoder network (stack of convolutional layers) which compresses the image into Latent vector / latent variables (low-dimensional bottleneck), which capture the core features, and then feed into a Decoder network (deconvolution / upsampling layers) reconstructs the image back to its original size and appearance.](./pics/autoencoder_workflow.png)\n",
    "\n",
    "### Denoising Autoencoders\n",
    "![computation graph for a denoising autoencoder. The clean input x is stochastically corrupted via C(\\tilde{x} | x), producing \\tilde{x}. An encoder then maps \\tilde{x} to h. A decoder g maps h to L. there is also a pointer from the clean input x to L.](./pics/denoisingArch_compGraph.png)\n",
    "- Define a corruption process, $C$: $$ C(\\tilde{x} \\mid \\vec{x}) $$\n",
    "- Autoencoder learns a *reconstruction distribution* $$ p_{\\text{reconstruct}}(x \\mid \\tilde{x}) $$\n",
    "1. Sample a training sample $x$\n",
    "2. Sample a corrupted version $\\tilde{x}$ from $C$\n",
    "3. Use $(x, \\tilde{x})$ as a training pair\n",
    "\n",
    "- De-corruption process results in learning a *distribution* <br> ![workflow of denoising autoencoder. Encoder (conv) takes the input image and outputs two vectors: a mean vector and a standard-deviation vector. From these comes a sampled latent vector. The decoder reconstructs the sampled latent vector back into the original input. By forcing the encoder to output a distribution instead of a point, the AE can both reconstruct inputs and generate new samples by drawing sample latent vectors from the learned latent distribution.](./pics/dae_workflow.png)\n",
    "\n",
    "### Variational Autoencoders (VAEs)\n",
    "- Associated with autoencoders by virtue of architecture\n",
    "    - Goal is to map inputs to latent space\n",
    "- Encoder: Learn parameters of variational distribution, $q(z \\mid x)$\n",
    "- Decoder: Sample (generate!) from learned distribution to reconstruct input\n",
    "\n",
    "![vae workflow: input x -> encoder -> latent space -> decoder -> output x'](./pics/vae_workflow.png)\n",
    "\n",
    "![Variational Autoencoder diagram, showing both its inference and generative paths in probabilistic terms. Dataset D in \"data space\" (x-space): real data lives on a complex manifold (the squiggly shape). Encoder q_{\\psi}(z|x): Maps each observed x up into a region of latent space (z-space), producing an approximate posterior. Prior p_{\\theta}(z): A simple, fixed distribution (e.g.\\ a standard Gaussian) over the entire latent space (the big grey ball). Decoder p_{\\theta}(x|z): Takes a sample z (either from the encoder or drawn from the prior) and maps it back down to data space, reconstructing or “generating” an x. The arrows thus illustrate the two halves of a VAE: Inference and Generation with the goal of making q_{\\psi}(z|x) align with the prior p_{\\theta}(z) while reconstructing samples accurately.](./pics/vae_visual.png)\n",
    "\n",
    "### Restricted Boltzmann Machines (RBMs)\n",
    "- Wholly undirected deep network\n",
    "    - Implementation of a probabilistic graphical model\n",
    "    - Each variable conditionally independent given neighboring nodes\n",
    "- Parameterized by energy function: $$ P(v, h^{(1)}, h^{(2)}, h^{(3)}) = \\frac{1}{Z(\\theta)}\\exp{(-E(v, h^{(1)}, h^{(2)}, h^{(3)}; \\theta))} $$\n",
    "- Sampling from deep RBMs is hard, but training is paradoxically easy\n",
    "\n",
    "![schematic of a deep Boltzmann‐machine (a multilayer, “restricted” Boltzmann network). bottom row (v1, v2, v3) is the visible units. Middle row (h_{1}^{(1)} to h_{4}^{(1)}) is first hidden layer. top row (h_{1}^{(2)} to h_{4}^{(2)}) is second hidden layer. Undirected edges only between adjacent layers (no intra-layer connections).](./pics/rbm_schematic.png)\n",
    "\n",
    "### Deep Belief Nets (DBNs)\n",
    "- Connections *between* layers, but not units *within* a layer\n",
    "- Arguably one of the first successful applications of modern deep learning\n",
    "    - Hinton 2006 and 2007\n",
    "- Often built from an RBM template\n",
    "- Training is nearly intractable\n",
    "    - Posterior has to be approximated through annealed importance sampling (AIS)\n",
    " \n",
    "![architecture of a Deep Belief Network. Top two hidden layers form an undirected pair (an RBM) that learns a joint representation. Lower layers (from hidden-2 → hidden-1 → visible) are directed downward, generating the observed variables. No lateral connections within any layer—only between successive layers.In other words, a DBN stacks an RBM at the top with a directed, generative network beneath to model P(visible, hidden)](./pics/dbn_arch.png)\n",
    "\n",
    "### Generative Adversarial Networks (GANs)\n",
    "- Yann LeCun: \"There are many interesting recent development in deep learning... The most important one, in my opinion, is adversarial training (also called GAN for Generative Adversarial Netowkrs). This, and the variations that are now being proposed, is the most interesting idea in the last 10 years in ML.\"\n",
    "\n",
    "- Game-theoretic approach to generative modeling\n",
    "- Two deep networks: a **generator** ($G$) and **discriminator** ($D$)\n",
    "\n",
    "![GAN setup. Generator F_G Takes a random noise vector as input, Passes it through a neural network to produce a generated example. Discriminator F_D Receives both real and generated examples, Tries to classify each as Real or Fake. During training, the generator and discriminator play a minimax “game”: The generator improves at fooling the discriminator, while the discriminator improves at spotting fakes—driving the model toward producing realistic samples.](./pics/gan_setup.png)\n",
    "- **Generator**\n",
    "    - Input: a random vector $z$\n",
    "    - Output: something as close to a “real” data point as possible\n",
    "- **Discriminator**\n",
    "    - Input: a \"real\" data point OR a synthetic example from $G$\n",
    "    - Output: 1 or 0 (real or fake)\n",
    "\n",
    "- Minimax \"game:\n",
    "    - Generator and Discriminator have competing objectives\n",
    "    - Goal is to find an equilibrium point\n",
    "    $$ \\min_{G}\\max_{D} \\mathbb{E}_{x\\sim P_{\\text{real}}}\\log{D(x)} - \\mathbb{E}_{z}\\log{(1 - D(G(z)))} $$\n",
    "        > $\\max_{D}\\mathbb{E}_{x\\sim P_{\\text{real}}}\\log{D(x)}$: Maximize the discriminator's likelihood of identifying a real data example <br>\n",
    "        > $\\min_{G}\\mathbb{E}_{z}\\log{(1 - D(G(z)))}$: Minimize the discriminator's ability to differentiate real data from the Generator exemplars\n",
    "\n",
    "![diagram lays out the adversarial training objectives in a GAN, split into two parallel pipelines. Left (real data): x \\sim p_{data} -> differential Discriminator D -> D(x) should be driven towards 1. Right (fake data): noise z -> differentiable generator G -> generated sample G(z) -> discriminator D -> D(G(z)) should be driven toward 0 by D, but toward 1 by G.](./pics/gan_visual.png)\n",
    "\n",
    "### VAEs versus GANs\n",
    "- VAEs: expectation over learned distribution results in blurring\n",
    "- GANs: samples from learned distribution, resulting in sharper images\n",
    "\n",
    "### Autoregressive (AR) Models\n",
    "- DALLE-1, in January 2021, was an autoregressive Transformer\n",
    "- Our good friends Appearance and State\n",
    "    $$ y_{t} = Cx_{t} + u_{t} $$\n",
    "    $$ x_{t} = Ax_{t-1} + Wv_{t} $$\n",
    "- Once you've learned A_{i}, you can generate a new x_{t}!\n",
    "\n",
    "![image generation](./pics/ar_model.gif)\n",
    "\n",
    "### Latent Diffusion\n",
    "- Closely related to VAEs, normalizing flows, and energy-based models\n",
    "- **Hard** to convert noise into structured data\n",
    "- **Easy** to convert structured data into noise\n",
    "\n",
    "![visual showing an input gradually being turned into noise](./pics/latentDiffusion_visual.png)\n",
    "$$ \\text{Data} \\quad \\xrightarrow{q(x_{t} \\mid x_{t-1}) = \\mathcal{N}(x_{t}; \\sqrt{1-\\beta_{t}}x_{t-1}, \\beta_{t}I)} \\quad \\text{Noise} $$\n",
    "$$ \\text{Noise} \\quad \\xrightarrow{p_{\\theta}(x_{t-1} \\mid x_{t}) = \\mathcal{N}(x_{t-1}; \\mu_{\\theta}(x_{t}, t), \\sigma_{t}^{2}I)} \\quad \\text{Data} $$\n",
    "\n",
    "- Similar to hierarchical VAE\n",
    "    - BUT all latent states have same dimensionality as input\n",
    "    - BUT encoder is a linear Gaussian model, rather than being learned\n",
    "- Results in a very simple objective\n",
    "    - No risk of posterior collapse (unlike GANs)\n",
    "- Numerous variations of LD\n",
    "    - Denoising Diffusion Probabilistic Models (DDPM)\n",
    "    - Noise-conditioned Score Networks (NCSN)\n",
    "    - Stochastic Differential Equations (SDE)\n",
    "\n",
    "![Diffused Data Distributions. illustrates how a data distribution gets gradually corrupted into pure noise through the forward diffusion process. 1) q(x_0): the original data distribution (with its multimodal “wiggles”). 2) q(x_1), q(x_2), ...: successive marginals after adding a bit more Gaussian noise at each step; You can see the original structure fading. q(x_T): after T steps, the distribution is essentially isotropic Gaussian noise.](./pics/diffusedDataDist_visual.png)\n",
    "\n",
    "### Large Language Models (LLMs)\n",
    "- Not unique generative models per se\n",
    "    - LLMs = very, very large Transformers\n",
    "    - Usually with autoregressive blocks at inference / decoding (multimodal LLMs have started integrating diffusers, e.g. GPT-4.1)\n",
    "    - Trained on city blocks’ worth of GPUs\n",
    "- Sometimes called “Foundation Models”\n",
    "    - ”Foundation Models” are only found on Terminus; elsewhere in the galaxy, they’re just “Sparkling Language Models”\n",
    "\n",
    "## Commercial generative models\n",
    "\n",
    "### Probably screaming into the void here, but...\n",
    "- There’s AI, and there’s AI\n",
    "- AI\n",
    "    - Large language or image models\n",
    "    - Trained on massive amounts of data with large numbers of parameters\n",
    "    - Does a frighteningly good job of mimicking humans at very specific tasks\n",
    "    - **Not intelligent**\n",
    "- AI\n",
    "    - Intelligence that isn’t human but made by humans, aka artificial\n",
    "    - Mimics humans very well at all possible tasks, even those it wasn’t trained on\n",
    "    - Nowhere in the 5-10 year roadmap\n",
    "\n",
    "### DALL-E, Midjourney, Stable Diffusion, Firefly\n",
    "![company logos](./pics/imageGen_logos.png)\n",
    "- Corporate backed text-to-image generators\n",
    "- Subscription fees\n",
    "- Open source options\n",
    "- Training data\n",
    "\n",
    "### Evolution of Image generators\n",
    "- DALL-E given the prompt “a muscular barbarian with weapons beside a CRT television set, cinematic, 8K, studio lighting.”\n",
    "    - April 2022: <br> ![muscular barbarian image from 2022 (not that good)](./pics/barb_2022.png)\n",
    "    - October 2023 <br> ![muscular barbarian image from 2023 (vastly better than 2022 generated image)](./pics/barb_2023.png)\n",
    "\n",
    "### GPT-4\n",
    "- Powers ChatGPT\n",
    "- “Attention is All You Need”, 2017: A “Transformer-style model pre-trained to predict the next token in a document, using both publicly available data (such as internet data) and data licensed from third-party providers.”\n",
    "- “Deep reinforcement learning from human preferences”, 2017: “The model was then fine-tuned using Reinforcement Learning from Human Feedback (RLHF).”\n",
    "- Several thousand GPUs + petabytes of data = ChatGPT\n",
    "\n",
    "### PaLM, Cerebras, LLaMA, Falcon, OpenHermes\n",
    "- Similar underlying architecture to ChatGPT\n",
    "- Billions (to trillions?) of parameters\n",
    "    - GPT-5 rumored to have ~2T parameters\n",
    "- Billions to trillions of training tokens\n",
    "    - PaLM 2 and LLaMA 2: 3.6T and 2T, respectively\n",
    "- Varying levels of openness\n",
    "    - Some pre-trained models on Huggingface\n",
    "    - An open LLM + RLHF (reinforcement learning from human feedback) + RLAIF (reinforcement learning from AI feedback) + DPO (direct preference optimization) = best bang for buck, outside of ChatGPT or similar\n",
    "\n",
    "## Technical, Ethical, and Legal Considerations\n",
    "\n",
    "### This is not unique to Generative AI\n",
    "- **We should always be considering the ethical and legal ramifications of our work**\n",
    "- But: given how widely available and easily accessed tools like ChatGPT are, and the hype surrounding them\n",
    "    - **There’s never been a better time to have these conversations**\n",
    "\n",
    "### Advantages of Generative AI\n",
    "- Already legion!\n",
    "- Democratize access to art and figure generation\n",
    "- Interactive, natural-language interfaces\n",
    "    - As opposed to arcane tricks and query optimization hacks with traditional search engines\n",
    "- Revealed clear weaknesses in our assessment protocols\n",
    "    - Educational assessment (i.e., grading) should not be contingent on whether or not you had access to a chatbot\n",
    "\n",
    "![visual showing how Variational Autoencoder can be used for de novo molecule design](./pics/vae_molecular.png)\n",
    "- New scientific discoveries around medicine, biology, chemistry, and biochemistry\n",
    "- Design new compounds (drugs, antibiotics, treatments) by teaching generative models about known ones\n",
    "- Keynote speaker at IOB Symposium in 2023 spoke about using LLMs to discover new proteins\n",
    "\n",
    "- Accessibility and interactivity\n",
    "- Original image (top left) interpolated along VAE latent distribution, producing different facial expressions <br> ![visual showing diff facial expressions generated on to a person in a given input image](./pics/gen_facialexp.png)\n",
    "- Virtual avatars, assistants, video gaming\n",
    "\n",
    "### Technical Issues\n",
    "![flowchart cycle on recursive training. Finite sample from Data^n -> problable events are overestimated and improbable events are underestimated -> approximate fitting in model^N -> probable events poison reality and tails shrink over time -> bad data in Data^n](./pics/recursiveTraining_flowchart.png)\n",
    "- Recursive model training\n",
    "    - As more information on the internet (images, text) is AI-generated, LLMs will ingest this data as part of their training, creating a recursive training loop\n",
    "    - [“The Curse of Recursion: Training on Generated Data Makes Models Forget”](https://arxiv.org/abs/2305.17493)\n",
    "\n",
    "- Examples of recursive model training\n",
    "\n",
    "![This composite visualization demonstrates how repeatedly sampling from a generative model and then retraining it on its own outputs causes the learned distribution to collapse. In the top histograms, two symmetric peaks gradually converge and lose one mode after successive resampling. The middle row of scatter plots shows two clusters merging into a single point as iterations increase. Finally, the MNIST grids at the bottom reveal that, by the twentieth generation, diversity disappears and the model produces essentially the same digit over and over.](./pics/recTrain_ex.png)\n",
    "\n",
    "### Legal issues\n",
    "- Copyright\n",
    "    - OpenAI, Midjourney most likely training on image datasets **without** permission from authors\n",
    "    - Currently in the US, AI-generated art cannot be copyrighted $\\rightarrow$ **potential boon for public domain!**\n",
    "- Plagiarism\n",
    "    - Simply: if you didn’t write/code/create it yourself, and you didn’t otherwise specify where it came from (and sometimes, even if you did), it’s plagiarism\n",
    "    - Is getting the answer from ChatGPT and presenting it as your own any different from getting the answer from your classmate and presenting it as your own?\n",
    "- Huge implications in professional fields, given current chatbot accuracy levels\n",
    "\n",
    "### Ethical and moral issues\n",
    "- Disinformation\n",
    "- Enabling/scaling abuse\n",
    "- Environmental concerns\n",
    "- Worker exploitation\n",
    "- Hidden costs of AI\n",
    "| Model name | Number of parameters | Datacenter PUE | Carbon intensity of grid used | Power consumption | CO₂eq emissions | CO₂eq emissions × PUE |\n",
    "|------------|----------------------|----------------|-------------------------------|-------------------|-----------------|-----------------------|\n",
    "| GPT-3      | 175B                 | 1.1            | 429 gCO₂eq/kWh                | 1,287 MWh         | *502 tonnes*      | 552 tonnes            |\n",
    "| Gopher     | 280B                 | 1.08           | 330 gCO₂eq/kWh                | *1,066* MWh         | *352 tonnes*      | 380 tonnes            |\n",
    "| OPT        | 175B                 | *1.09^2*          | *231 gCO₂eq/kWh*                | *324 MWh*           | 70 tonnes       | *76.3 tonnes^3*           |\n",
    "| BLOOM      | 176B                 | 1.2            | 57 gCO₂eq/kWh                 | 433 MWh           | 25 tonnes       | 30 tonnes             |\n",
    "- Comparison of carbon emissions between BLOOM and similar LLMs. Number in *italics* have been inferred based on data provided in papers describing the models.\n",
    "\n",
    "![iceberg metaphor for AI: the small “tip” above the water lists flashy, consumer-facing capabilities (chat on any topic, realistic image generation, homework help, even hints of AGI), while the vast submerged portion reveals the deeper, hidden downsides—misinformation, propaganda and bias, cultural homogenization, privacy and copyright violations on one side, and on the other the systemic costs: mass data harvesting, underpaid labor, environmental impact (huge energy, water, carbon and rare-metal footprints), rising barriers to entry, and the erosion of human practices.](./pics/iceberg_genAI.png)\n",
    "\n",
    "### Philisophical issues\n",
    "- Novelty\n",
    "- Is the content generated from ChatGPT / Midjourney **new**?\n",
    "- The “tool” analogy\n",
    "    - Generative AI is inherently neither good nor bad, but dependent on its application\n",
    "- In 1999, French cultural theorist Paul Virilio wrote, *\"When you invent the ship, you also invent the shipwreck; when you invent the plane you also invent the plane crash; and when you invent electricity, you invent electrocution... Every technology carries its own negativity, which is invented at the same time as technical progress.\"*\n",
    "\n",
    "### I don’t have answers\n",
    "- But we **have to** think about the pitfalls, and especially about **who the technology will impact**\n",
    "\n",
    "```\n",
    "Which of the following is NOT a generative model?\n",
    "    a. VAEs\n",
    "    b. GANs\n",
    "    c. AR Models\n",
    "    d. Logistic Regression // correct\n",
    "    e. Transformers\n",
    "    f. Gaussian Naive Bayes\n",
    "    g. RBMs\n",
    "    h. DBNs\n",
    "```\n",
    "\n",
    "## Conclusions\n",
    "- Generative modeling\n",
    "    - Learn a *distribution* instead of a decision boundary\n",
    "    - Can still be used for classification\n",
    "    - Usually requires more data than discriminative models\n",
    "- Deep generative modeling\n",
    "    - DBNs, RBMs, Denoising & Variational Autoencoders, GANs, AR models, LD\n",
    "    - All ways of learning a generating distribution from data in deep neural architectures\n",
    "- Deployments of generative AI\n",
    "    - Commercial products (ChatGPT, Stable Diffusion, Midjourney, DALL-E)\n",
    "    - Possibilities, advantages, moral/ethical/legal/philosophical considerations\n",
    "    - Consider the possible use-cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5d1630-a425-451d-9df7-429681f3aa23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
